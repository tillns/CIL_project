-login to cluster with ssh user@login.leonhard.ethz.ch

-if you want to use an ssh key, add it to your ssh agent with ssh-add ~/.ssh/id_rsa (google how to generate key) and copy it to cluster with ssh-copy-id user@login.leonhard.ethz.ch

-load some modules:
module load gcc/4.8.5 python_gpu/3.6.4 hdf5 eth_proxy
module load cudnn/7.4

-install virtual environment via: 
pip install --user virtualenvwrapper
source $HOME/.local/bin/virtualenvwrapper.sh
-Create a venv via:
mkvirtualenv "env-name"
-You may exit and reenter the venv via
deactivate
workon "env-name"

Add the following lines to your .bashrc (got to your home folder: "cd", then "vim .bashrc", press "i" and insert the following lines):

module load gcc/4.8.5 python_gpu/3.6.4 hdf5 eth_proxy
module load cudnn/7.2
source $HOME/.local/bin/virtualenvwrapper.sh
workon "env-name"
(exit vim mode via "Esc" and ":wq"). This loads the specified modules and starts the virtual environment each time when logging in. You may experiment with different versions.
Inside the virtual environment, you can install everything once and it should stay there. We have the following lines in a python file setup.py:
from setuptools import setup, find_packages
"""Setup module for project."""

setup(
        name='mp19-project-skeleton',
        version='0.2',
        description='Skeleton code for Machine Perception Dynamic Gesture Recognition project.',

        author='Emre Aksan',
        author_email='eaksan@inf.ethz.ch',

        packages=find_packages(exclude=[]),
        python_requires='>=3.5',
        install_requires=[
                # Add external libraries here.
                'tensorflow-gpu==1.12.0',
                'numpy',
                'patool',
                'opencv-python',
        ],
)
Not sure if it works but try to copy this into a python file and run it inside the venv via "python filename.py". Maybe also change the version. If it doesn't work, simply install those things manually, e.g.
pip install tensorflow-gpu==2.0.0-alpha0
for installing tf 2 etc.

To test whether your environment is working, create the following test script test.py in your home:

import tensorflow as tf
c = tf.constant('TF version: {}'.format(tf.__version__))
with tf.Session() as sess:
    print(sess.run(c))
Now you can run this script by typing the following command (make sure your virtual environment is activated at this point):

bsub -n 1 -W 4:00 -o log_test -R "rusage[mem=1024, ngpus_excl_p=1]" python test.py





Following is the full website of Machine Perception, explaining the behavior. It might contain some copy-paste errors as well as irrelevant things and it definitely misses the --user flag for the venv installation command:



The agents should have the capability of understanding the environment and infer the usersâ€™ state and intentions over time to provide a better user experience. Hence, visual understanding is one of the core research problems with a vast range of applications including machine perception and user interface design. A visual recognition system could be considered as one of the building blocks of such a pipeline. In this project, we focus on a particular case where the agent is required to recognize sign language commands from videos.

We are providing you with a customized version of the ChaLearn dataset. The data contains RGB, depth, segmentation mask and skeletal information for videos depicting gestures drawn from a vocabulary of 20 Italian sign gesture categories. Our data consists of short clips where a subject performs a gesture. See above for an example frame. You will also be given sample code that helps you load, preprocess the data and train a very simple model. You are expected to modify it or implement your own code.

TASK
You are required to train and evaluate a model that recognizes individual gestures from a clip by using neural networks. For a given video snippet, your model should predict the category of the gesture performed in the video. Please note that overfitting is one of the main challenges of this project. Hence you should control model complexity and consider regularization techniques.

LITERATURE
Molchanov et al. (2016) Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks
Tran1 et al. (2018) A Closer Look at Spatiotemporal Convolutions for Action Recognition
Pigou et al. (2018) Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video
DATA DESCRIPTION
You can find it at /cluster/project/infk/hilliges/lectures/mp19/project1/ on Leonhard. The data is stored using the TFRecord format. We already provide you training, validation and test splits where each split consists of multiple tfrecord files enabling parallel IO. You are welcome to change training and validation splits. Note that you do not need to copy the data to your local workspace unless you want to modify the data files. The provided skeleton code loads the data and applies preprocesing and normalization (see dataset.py) on the fly.

Each record, i.e., clip sample, contains the following fields:

id: an intiger id of the sample. While it is not reuqired by the training and validation splits, it is useful in creating the final prediction file,
label: label of the sequence taking values between 0 and 19,
length: length of the sequence, i.e., number of frames,
depth: list of depth images (length x height x width x 1),
rgb: list of rgb images (length x height x width x 3),
segmentation: list of binary masks (length x height x width x 3),
skeleton: list of flattened skeleton joint positions (length x 180).
Each clip contains 50-150 frames. Frames (images) are resized to 80x80. The test data follows exactly the same scheme, but the label value is always -1.

Please note that the splits consist of different subjects and hence the complexity is slightly different. The test performance, for example, is always better than the validation performance. However, the relative changes in the validation performance are usually translated to the test performance.

CODE DESCRIPTION
A sample code for loading the data, training a model, saving, restoring and getting predictions on the test data.You can download the source code by running

wget --user=mp-2019 --ask-password https://ait.ethz.ch/teaching/courses/2019-SS-Machine-Perception/downloads/projects/project1_skeleton.zip
unzip project1_skeleton.zip -d project1
cd project1
Use the same password as you do when downloading slides from the website.

In the following each code file is explained in more detail. We recommend going through each of the source files to understand the code base in-depth. It contains many comments, so that you can quickly get an idea of how to implement your own model.
config.py: configuration file for experiment,
dataset.py: dataset class creating Tensorflow routines to load tfrecords, apply preprocessing and normalization,
model.py: provides an abstract base class, CNN and RNN model classes,
training.py: training script loading data, building models, running training and saving checkpoints,
restore_and_evaluate.py: evaluation script loading test data and a pre-trained model, evaluating the model on test data and creating submission files,
utils.py: provides the functionality to create submission files. If you add a new code file, please update the file list in this code,
Skeleton.py: provides the Skeleton class creating the skeleton image. This is not part of the sample code. You can find an example use in the __main__ of dataset.py. If you are interested in custom solutions you can also find details of the skeleton data on ChaLearn page.,
setup.py: project setup script installing the required libraries,
dataset_numpy_to_tfrecord.py: an auxiliary script for storing data in tfrecords format as expected by the skeleton code,
 The skeleton code is copyright-protected. You are free to use and modify it. However, do not share your modified version with other groups or make it publicly available.
SUBMISSION FORMAT
For every sample in the test set, the submission file should contain two columns: Id and y.

The file should contain a header and have the following format:

Id,y
1,12
2,13
3,7
...
The sample code creates the csv submission file for you. Moreover, the code files you used to create that particular submission are archived. The submission system expects you to upload both the csv and zip files. The sample code provided for this project ensures that the submitted code files are enough to rerun your experiment by simply calling python training.py. If you add new code files do not forget to edit utils.py. If you use command-line arguments or other custom stuff, please provide the instructions in a readme file.

Please keep in mind that, as a group, you have a limited number of submissions as stated on the submissions page.

EVALUATION
The evaluation metric for this project is the prediction accuracy. That is the number of correct prediction over the entire test set.

GRADING
We provide you with one test set for which you have to compute predictions. We have partitioned this test set into two parts and use it to compute a public and a private score for each submission. You only receive feedback about your performance on the public part in the form of the public score, while the private leaderboard remains secret. The purpose of this division is to prevent overfitting to the public score. Your model should generalize well to the private part of the test set.

When handing in the task, you need to select which of your submissions will get graded and provide a short description of your approach. For example, you can type "50 layer fancy resnet model for this awesome project". We will then compare your selected submission to two baselines (easy and hard). As we want to encourage novel solutions and ideas, we also ask you to write a short report (3 pages, excluding references) detailing the model you used for the final submission and your contributions. Depending on this, we will weigh the grade determined by your performance w.r.t. the baselines. Hence, your final grade depends on the public score and the private score (weighted equally), on your submitted code and on your report.

The following non-binding guidance provides you with an idea on what is expected to obtain a certain grade: If you hand in a properly-written description, your source code is runnable and reproduces your predictions, and your submission performs better than the easy baseline, you may expect a grade exceeding a 4. If your submission performs equal to or better than the hard baseline, you may expect a 5.5. The top 2 submissions will achieve 6 and everything in-between the second best submission and the hard baseline will be linearly interpolated. The grade computed as mentioned above can go up or down, depending on the contributions of your project. If you passed the easy baseline, however, your final grade cannot go lower than 4 after the weighting.

Make sure that you properly hand in the required materials, otherwise you may obtain zero points.

ENVIRONMENT SETUP
CUDA and cuDNN are already installed on Leonhard, so you won't have to deal with this. If you want to test something locally and don't want to install CUDA/cuDNN, you could think about installing the CPU version of TensorFlow.

There are some TensorFlow binaries pre-installed on TensorFlow, but not the latest versions. Thus, we recommend installing TensorFlow yourself in a virtual environment. You can use either virtualenvwrapper or Miniconda. Because there is a file amount limit on the home directory, don't use Anaconda.

The provided skeleton-code was tested with Python 3.6, TensorFlow , CUDA 9.0 and cuDNN 7.2. In the following it will be explained how to install all required dependencies assuming you use virtualenvwrapper, but instructions for Miniconda should be similar. If you need help with the installation, visit the official TF website or ask on Piazza.

Login to Leonhard cluster:

ssh user@login.leonhard.ethz.ch
You need to load the required modules including python, CUDA and cuDNN provided on the server:

module load python_gpu/3.6.4 hdf5 eth_proxy
module load cudnn/7.2
Install virtualenvwrapper if you haven't done so yet:

pip install virtualenvwrapper
source $HOME/.local/bin/virtualenvwrapper.sh
Create a virtual environment:

mkvirtualenv "env-name"
The virtual environment is by default activated. You can disable or enable it by using

deactivate
workon "env-name"
Go to you project folder and run the following to install the required dependencies:

python setup.py install
It might be a good idea to add these commands to your bashrc file, so you don't have to load the environment every time you login to the node. Add these lines to the end of ~/.bashrc:

module load gcc/4.8.5 python_gpu/3.6.4 hdf5 eth_proxy
module load cudnn/7.2
source $HOME/.local/bin/virtualenvwrapper.sh
workon "env-name"
USING LEONHARD
Leonhard is a shared resource, so please be considerate in its use. This means for example to request as much memory as your job requires, but not an overly excessive amount that your program does not end up using. Also, you are only allowed to use 1 GPU at a time per group.

To run your training scripts on Leonhard, you submit job requests, which are placed in a queue and executed once a GPU is free. To avoid congestion towards the deadline, we highly encourage you to start working on the project as early as possible.

To test whether your environment is working, create the following test script test.py in your home:

import tensorflow as tf
c = tf.constant('TF version: {}'.format(tf.__version__))
with tf.Session() as sess:
    print(sess.run(c))
Now you can run this script by typing the following command (make sure your virtual environment is activated at this point):

bsub -n 1 -W 4:00 -o log_test -R "rusage[mem=1024, ngpus_excl_p=1]" python test.py
Explanation of options:

-n 1 Request this many CPU cores (here 1).
-W 4:00 Wall time (here 4 hours). After running for this many hours your job is automatically cancelled. This also determines in which queue your job lands.
-o log_test After completion of your script, its output is written into this file stored in the current directory.
-R Requested resources. mem is in MB and per requested CPU core, i.e. here we are requesting 1 GB. ngpus_excl_p is the amount of GPUs requested (must always be 1).
When you submitted the job, you can check its status via bjobs (may be you will find watch -n 1 bjobs useful). When the job is waiting in the queue its status is PEND, once it is running it will change to RUN. Here you can also find a unique job id. You can check some details of the job via bjobs -l <id> (e.g. how much memory your job used). Once the job finished you should find a file log_test in the directory where you submitted the job from. If you can find the statement TF version: in that file, your environment is ready.

For this project, 8 CPUs and 4096 MB of memory per CPU should typically be enough. The following command is probably a good idea:

bsub -n 8 -W 12:00 -R "rusage[mem=4096, ngpus_excl_p=1]" python training.py
You can increase the memory and wall time if your model requires more time and resources.
WRITING
You can use your home directory to store trained models and tensorboard logfiles. If this is not enough, you may create a folder in /cluster/project/infk/courses/machine_perception_19/your_group_name. Again, the available disk space is limited, so please use it considerately. Note that this directory will be deleted after the end of the course.

SAMPLE SUBMISSION
Follow the instructions to set up the environment on Leonhard before running any of the provided code. The provided code implements a dummy model and produces a sample submission file that you can submit to check whether everything is working as it should. To do so, run the following command in the directory where you unzipped the code files to:

bsub -n 8 -W 12:00 -R "rusage[mem=4096, ngpus_excl_p=1]" python training.py
This will create a new experiment directory under ./runs/. You can find the details in config.py. The model and model configuration are stored into this experiment directory so that we can re-load a pre-trained model later.

Once the training is done, your model is automatically evaluated on the test split. You can find the submission .csv and .zip files under the experiment directory. The .csv file contains the predictions for all test samples. The zip file contains a copy of the code files. You can upload both to the submission system. The code files might be helpful later when you want to check which code produced an already submitted result. Pro-tip: It is helpful to always commit before you run a new training.

Once you submit the .csv file from the skeleton code, you should get a similar result to the "sample baseline" that is already in the leaderboard. This means everything works as expected and you can start working on you are now ready to start on your own model implementations!

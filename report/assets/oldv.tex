\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{siunitx}

\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\newcommand\TODO[1]{\textcolor{red}{#1}} % for all TODOs
\usepackage{pdfpages}

\begin{document}
\title{Computational Intelligence Lab Report\\ {\large Galaxy Image Generation}}

\author{Till Schnabel, Sven Kellenberger, Hannes Pfammatter, Michelle Woon \\group: Galaxy Crusaders, Department of Computer Science, ETH Zurich, Switzerland}

\maketitle

\begin{abstract}
This paper explores various machine learning models for generating sparse, high-resolution cosmology images. These models will be described and the motivation for their usage explained.

The contribution of this paper is twofold. Beside the design of models for cosmology image generation, a similarity function which is part of our data is learned. 

The generative models are compared to a simple baseline model and evaluated by estimating the values of the similarity function.

As our conclusion, we found that extracting the most important features from the images and designing generative models for these features only produced the best results. Our approximation of the similarity function achieved the $2^{nd}$ best result on both public and private Kaggle test datasets.


%This paper explores various ways for generating as well as approximating a similarity function for high resolution cosmology images. The methods solving the latter issue is evaluated via Kaggle\cite{Kaggle}, achieving 2nd place on the public leaderboard.

%For the similarity score approximator, an advanced Random Forest (RF) method is evaluated and compared to a Convolutional Neural Network (CNN) and a simpler RF approach. The structure of our CNN will be introduced and analyzed. This paper also provides insights to the advanced RF used, such as how and why it performed better than the CNN. We will also discuss the image preprocessing and data augmentation used for all learning methods introduced in this paper. 

%For the generation of the highly space cosmology images, an adhoc method, a Variational Autoencoder (VAE), a conditional Deep Convolutional Generative Adversarial Network (cDCGAN) and also a PixelCNN \TODO{(leave this?)} approach are all explored. We will discuss and evaluate each method in this paper. We present our cDCGAN on k-means clustered stars method.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Generative modeling is not a new area of research. However, with the recent advent of deep generative models the research interest in this area has increased drastically. \\ 
In this project, we study the generation of galaxy images. As our galaxy images are sparse, high-dimensional data, this has shown to be an interesting problem suitable for the application of mathematical methods and machine learning models.

We split our project into two parts. First, we designed multiple machine learning models for galaxy image generation. Our starting point was a DCGAN capable of generating galaxy images with high resolution. As our model was not able to capture important features such as stars with enough detail, we decided to focus on generating these features only. For the generation of stars we designed a VAE model, a DCGAN model and a PixelCNN. Our final solution consists of a conditional DCGAN that is able to generate several classes of stars that were found by clustering.

Second, we designed a CNN model and used an off-the-shelf random forest regression model to learn the similarity function which is part of our data. Finally, we evaluated our generative models by estimating the similarity scores of the generated images. 

As our models are quite diverse, we refer to section \ref{sec:models_methods} for the discussion of related work.


%Similarity scorers are useful for finding similar images to a given image. In this paper we evaluate RFs and a CNN as approximators of a similarity function for a cosmology image. The simpler RF does not make use of FFT, while the advanced RF and the CNN approximators use it. Despite being the more "sophisticated" method of the two advanced methods, the CNN performs worse in both time and mean absolute deviation. Our advanced RF approximator reaches a mean absolute deviation of 0.09542 on the public and 0.10463 on the private test set, scoring slightly better than the CNN.

%Image generation is not a novel problem, however the task to generate sparse high resolution data has proven to be difficult for generative models. Hence, to solve this issue for generating cosmology images, we split the task into the generation of small $28\times28$ patches of single stars only, extracted from the given galaxy images, and the distribution of the results on a plain high resolution background image. We keep the distribution close to the given data.




%%%%%%%%% TODO
%\TODO{Mention the best generator. (cDCGAN?)}
%%%%%%%%%

%\TODO{Related work is not in the guideline, but I guess they add them in "Models and Methods"}
%\section{Related Work}
%To prevent overfitting, Srivastava et al.\@ \cite{Dropout} introduced Dropout in 2014. The idea is that e.g.\@ a fully connected layer is followed by such a dropout layer that sets a configurable percentage of output neurons to zero during training, thus effectively omitting their impact on the final output. Consequently, the preceding layer is less likely to overfit to training data, because it is forced to learn a more meaningful representation of the data using all neurons.

%In 2015, Ioffe et al.\@ \cite{BatchNorm} introduced BatchNorm --- a normalization technique that takes as input the batch of data and normalizes across this batch. Other than for instance normalization during pre-processing, a BatchNorm layer contains learnable mean and variance, s.t.\@ the network may adjust to covariate shifts and vanishing gradients in between layers. Today, it is widely used in classification and other networks, to speed up the learning process.

%Also in 2015, He et al.\@ \cite{ResNet} released their ResNet architecture, which uses so-called ``Skip connections'' that allow for far deeper architectures than practical ones before. The output of a building block of a few convolutional layers is summed with this block's original input, which allows the network to skip unwanted blocks. This process, they call ``Residual learning''. The problems of deep architectures like zero gradients were compensated by this new invention.

\section{Models and Methods} \label{sec:models_methods}
This section briefly describes the models and methods which were used for this project and explains the motivation for their usage. The first part contains the discussion of the models for cosmology image generation. The second part discusses the approximation of the similarity function.

%The task into a regression task and a generation task. We used the models for regression to evaluate the models for generation. 
%The dataset used for this project was downloaded from the galaxy image generation competition on Kaggle \cite{Kaggle}. 
%It consists of 1200 labeled images with labels indicating whether an image is a galaxy image or not and 9600 scored images with scores indicating the similarity of an image to the prototypical cosmology image. We used the scored images to learn the similarity function. To train our models, we either used the labeled images only or the labeled images and a fraction of the scored images.

% \subsection{Data}

%It comes with 3 different sets of images:
%\begin{itemize}
%    \item Labeled: The images in this set are labeled on whether they are cosmology images or not.
%    \item Scored: The images in this set are scored from 0 to 8 on their similarity to a prototypical cosmology image. A score like 2.61 means that the image almost coincides with said prototype. The higher the score the more similar the image is.
%    \item Query: Images to assign scores to for the competition.
%\end{itemize}




\subsection{Approximation of the similarity function}
\subsubsection{Random forest regression} 
In this section, the random forest regression baseline is discussed. By improving on this baseline we achieved our best results for approximating the similarity function. \\
Random forests were introduced in 1995 by Ho \cite{Ho} and further developed in 1999 by Breiman \cite{Breiman}. In this work, they were used as an off-the-shelf model for regression. Histograms of image properties were used as input features. \\
We started with a histogram of pixel intensities and consider this our baseline model. By using histograms of different image properties, we were able to improve the accuracy of the regression as measured by the mean absolute error (MAE) by a factor of approximately 3. The results are shown in Table \ref{tab:RF}. In the following paragraphs, the results are detailed. \\ 
In 2007 Bosch et al. \cite{Bosch} used random forests for image classification. They captured information on shape by computing histograms of oriented gradients (HOG) of edges inside image regions. We computed a similar histogram for the entire image and combined it with the histogram of pixel intensities which improved the accuracy of the regression. \\ 
What worked best was a histogram of the power spectral density (PSD). To capture the PSD in a histogram, we used the fast Fourier transform (FFT) and applied a log transformation to the power spectrum. \\
We were not able to improve the results by combining the histogram of the PSD with other histograms. We tried to get better accuracy by adding histograms of regions of interest (ROI) where we filtered out either high or low frequencies but this did not substantially improve the results. \\

\subsubsection{CNN} % hard baseline
For the neural network approach for image classification, we used a deep convolutional architecture. We experimented a lot with the specific layers. Most experiments were performed on lower-resolution images ($125\times125$ and $250\times250$), because simple patterns like the star distribution could similarly be captured on this resolution while saving lots of computation time. Good settings were then applied to the original data. Our most important discoveries were:
\begin{itemize}
    \item Batch normalization (Ioffe and Szegedy \cite{BatchNorm}) greatly stabilizes and thus accelerates the training.
    \item Because the final score is linear between 0 and 8, a sigmoid layer times 8 in the end makes it harder for the network to predict the score correctly and is thus counter beneficial.
    \item A fast fourier transform (FFT) preprocessing improves and stabilizes the training.
    \item Many convolutions but few parameters lead to the best results, because more convolutions mean a greater receptive field and a lot of parameters quickly cause overfitting on the sparse images.
    \item We did not find residual connections (He et al.\@ \cite{ResNet}) to be helpful.
    \item Dropout (Srivastava et al.\@ \cite{Dropout}) reliably prevents the network from early overfitting.
\end{itemize}
Our final architecture can be seen in figure \ref{fig:nn_classifier_arch}.  Not mentioned is the padder that pads the intermediate code from $125\times125$ to $128\times128$ spatial resolution. The number of features were raised by a factor of 2 every stacked convolutional block (colored in blue) until the maximum of 32 was reached. In total, there were eight of these blocks. No bias was used. The network had a total of only \SI{37376} parameters, \SI{37120} of which trainable. As mentioned, the images were preprocessed with FFT. During training, the data were augmented with horizontally and vertically flipped with a probability of \SI{0.5} as well as randomly shifted in height and width by up to \SI{20}{\percent}. We reached a score of \SI{0.16792} on the public and \SI{0.18781} on the private part of the test set, after training the model for 140 epochs.
\begin{figure}
    \centering
    \includegraphics[width=0.4\columnwidth]{assets/nn_classifier_arch.pdf}
    \caption{Illustration of the CNN classifier architecture.}
    \label{fig:nn_classifier_arch}
\end{figure}

\subsection{Galaxy Image Generation}
\subsubsection{Adhoc Generator (AG)} % easy baseline
The cosmology images are in essence, white stars on a black background, as such we were able to use a simple tiling method for our easy baseline. Using the available labelled data, we detect stars in all the cosmology images by finding contours in the image. Doing this also allows us to find the minimum and maximum amount of stars in a cosmology image. 

Each image to generate first starts out as a black destination image. Then, for each image to generate a random number between the minimum and maximum amount of stars is selected. For each star to place into the image, a random source cosmology image is chosen and then a random star from that source image is taken. That star is then placed on a random spot in the destination image.

\subsubsection{Large DCGAN} In this section, the deep convolutional generative adversarial network (DCGAN) which was used to generate entire galaxy images is described. \\
The DCGAN was introduced in 2015 by Radford et al. \cite{Radford}. It is a generative adversarial network (GAN) where both generator and discriminator are convolutional neural networks (CNNs) with architectural constraints. \\
Our implementation is based on on a reference implementation on the TensorFlow website \cite{dcgantf} as well as on the original paper and implementation. As proposed by Odena et al. \cite{Odena} nearest neighbor interpolation and convolution were used instead of transposed convolution. \\
For training, all labeled images and all scored images with similarity score greater or equal than 2.61 were used. The pixel intensities were normalized to the range [-1, 1]. The images were padded to dimensions 1024 x 1024 to simplify upsampling and strided convolutions. \\
Although it was possible to generate entire galaxy images, the results were not satisfactory. As shown in Figure \ref{fig:dcgan_stars}, the large DCGAN did not manage to catch the shape of stars in detail. Table \ref{tab:gen_MSS} shows the estimated similarity scores for these galaxy images.
\subsubsection{VAE on stars} % hard baseline
In this section, the variational autoencoder (VAE) which is used to generate images of stars is described. \\
The variational autoencoder was introduced in 2014 by Kingma and Welling \cite{DBLP:journals/corr/KingmaW13}. Our implementation is based on the original paper as well as on the tutorial on variational autoencoders by Doersch \cite{doersch2016tutorial} and on a reference implementation on the TensorFlow website \cite{cvaetf}. \\
The VAE allows to encode data points to low-dimensional latent representations  $\sim\mathcal{N}(0, I)$ and to generate new data points by decoding arbitrary latent vectors $\sim\mathcal{N}(0, I)$. This is achieved by learning to map each data point x to a Gaussian distribution $q_{\phi}(z|x)$ determined by $\mu$ and $\sigma$ and each latent vector z to a Bernoulli distribution $p_{\theta}(x|z)$ determined by $p$, which is done by maximizing the expectation lower bound (ELBO):

\[
-D_{KL}(q_{\phi}(z|x)||\mathcal{N}(0, I)) + \mathbb{E}_{q_{\phi}(z|x)}(log(p_{\theta}(x|z)))
\]
\\
Our implementation approximates the expectation with the binary cross-entropy loss function. For the Kullback-Leibler divergence, the analytic expression from appendix B of the original paper is computed:

\[
\frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_{j})^{2}) - (\mu_{j})^{2} - (\sigma_{j})^{2})
\]
\\
For training, only the labeled images are used. Stars are extracted and centered inside images of size 28x28. The pixel intensities are normalized to the range [0, 1]. \\
Because stars are shaped in a similar fashion, a MLP with a single hidden layer of size 500 is used for both the probabilistic encoder $q_{\phi}(z|x)$ and the probabilistic decoder $p_{\theta}(x|z)$. The parameters of the MLPs are denoted by $\phi$ and $\theta$. The latent dimension is set to 16. \\
Figure \ref{fig:vae_interpolation} shows linear interpolations between latent vectors to demonstrate that the model works well. To create galaxy images, generated star images are distributed randomly inside an image with black background. The number of stars per image is normally distributed and estimated from the labeled images. Table \ref{tab:gen_MSS} shows the estimated similarity scores for these images.


\subsubsection{cDCGAN on Patches}
We found that a GAN that has to produce high resolution images is very hard to train. While our results from the DCGAN do follow the pattern from the cosmology data, the stars generally lack the necessary precise detail to make them look realistic. Hence, we explored a mixture of the straight-forward Adhoc with the DCGAN approach. We trained a smaller GAN on the $\SI{28}{}\times\SI{28}{}$ patches of the extracted stars. Since those patches resemble the MNIST (LeCun and Cortes \cite{lecun-mnisthandwrittendigit-2010}) data set in both spatial size and depth, we could simply adopt an existing architecture for our GAN. For the generation of the high resolution data, the generator was fed some random latent code and the resulting star patches were put into the black background image in random fashion similar to the Adhoc method.

While this approach already produced quite convincing results, we decided to go one step further. We found that there were different kinds of stars inside the cosmology images; some very bright, some higher than broad, some very dark, some completely round. However, considering our large data set of around 15k star patches, most of them looked very similar, meaning they could potentially be divided into some small amount of individual classes. We trained a simple deep convolutional autoencoder (DCAE) on the star patches for 400 epochs. 
%We didn't have to worry about overfitting because we only wanted to cluster the same training data.
We then applied k-means clustering on each patch's latent code produced by the DCAE's encoder. That way, the stars were separated into five distinct classes. We did not have to worry about overfitting the DCAE, because it was only used on the data it was trained on.

Afterwards, we used the clustered data to train a conditional DCGAN (cDCGAN). Next to the image/latent code, the generator/discriminator are also fed the class label belonging to the star. To prevent the GAN from overfitting, we generated complete images every few epochs and included the best RF and CNN classifier models to score them. But we could not simply distribute the stars at random in the black background image, since this would not be an objective measurement for the generator score.
Hence, we measured the number of each star class on the given cosmology images and approximated it with a normal distribution bound to unsigned integers, because there cannot be a fraction number or a negative amount of stars in an image. We assumed the positioning of a star to be uniformly random. In order to be deterministic in the final result, we used a prototype cDCGAN model to distribute the stars following the specified distributions of number and positioning. Each time a set of \SI{100} images was produced, the best RF and CNN models scored them. We repeated this process \SI{2000} times and saved those random numbers that produced the highest score. With all the numbers saved, the next cDCGAN could then be trained with a deterministic validation procedure. 

The architecture of the cDCGAN was also adopted from \footnote{\url{https://www.tensorflow.org/beta/tutorials/generative/dcgan}}, but this time no resolution adjustments had to be made, only the conditional property had to be added. The architecture of the conditional generator can be seen in figure \ref{fig:cgen_arch}. It uses a total of \SI{3263040} parameters, of which \SI{3212480} are trainable. Figure \ref{fig:clustered_generated_stars} shows a generated example of the five different star classes. The discriminator uses \SI{281857} parameters, with \SI{269313} trainable. Its architecture is illustrated in figure \ref{fig:cdis_arch}. The number of parameters is quite high and could have probably been lowered without loss of quality. However, since the generation of such small images is very stable, no architectural experiments were conducted. The cDCGAN was trained for 185 epochs, which lasted only about half an hour, from which around half the time was spent on validation. 
\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{assets/cgen_arch.pdf}
    \caption{Illustration of the architecture of the conditional generator.}
    \label{fig:cgen_arch}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{assets/clustered_generated_stars.png}
    \caption{Samples of the five different star classes generated by the cDCGAN.}
    \label{fig:clustered_generated_stars}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{assets/cdis_arch.pdf}
    \caption{Illustration of the architecture of the conditional discriminator.}
    \label{fig:cdis_arch}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{assets/star_ensemble_large.png}
    \caption{Random selection of stars generated by the large DCGAN.}
    \label{fig:dcgan_stars}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{assets/vae_interpolation.png}
    \caption{Linear interpolation between vectors in the latent space of the star variational autoencoder.}
    \label{fig:vae_interpolation}
\end{figure}

\subsubsection{PixelCNN}
For the PixelCNN, we adopted the code from \footnote{\url{https://github.com/anantzoid/Conditional-PixelCNN-decoder}}, which follows the work from van den Oord et al.\@ \cite{PixelCNN}, and replaced the MNIST with our own star patch data set. We evaluated a conditional and unconditional setting. The results looked strictly worse than those of the (c)DCGAN in both settings, so we did not bother to include the code any further into our existing project.



\section{Results}
\label{sec:results}
\subsection{Similarity Function Approximators}


%\begin{table}[htbp]
%  \centering
%  \begin{tabular}[c]{|l||r|r|r|}
%    \hline
%        & \# features & MAE & STD \\
%    \hline
    
%    Pixel intensity + oriented gradients             &      90 + 36     &   0.177 & \\
%    Power spectral density             &    48       &  0.104 & \\
%    Power spectral density + ROI           &     48 + 48      &  0.086 & \\
%    \hline
%  \end{tabular}
%  \caption{Random Forest regression using different features.}
%  \label{tab:RF}
%\end{table}

\begin{table}\centering
\begin{tabular}{lrrr}
\toprule
& \# features & MAE & MAE \\
& & ~(pub.) & ~(priv.) \\
\midrule
Pixel intensity (baseline) & 32 & 0.258 & 0.287  \\
Pixel intensity + oriented gradients & 32 + 36 & 0.183 & 0.203 \\
Power spectral density & 48 & 0.107 & 0.124 \\
Power spectral density + ROI & 48 + 48 & 0.095 & 0.105 \\
\bottomrule
\end{tabular}
\caption{Mean absolute error (MAE) for random forest regression using different features.}\label{tab:RF}
\end{table}


\begin{table}\centering
\begin{tabular}{lrr|rr}
\toprule
Model & MAE & STD &  MAE &  MAE \\
 & (loc.) & (loc.) & (pub.) & (priv.) \\
\midrule
RF (baseline) & & & 0.258 & 0.287 \\
CNN & & & 0.168 & 0.188 \\
RF (best) & & & 0.095 & 0.105 \\
\bottomrule
\end{tabular}
\caption{Mean absolute error (MAE) and standard deviation of the absolute error (STD) of our models approximating the similarity function.}\label{tab:SFA_MSS}
\end{table}


%\begin{table}[htbp]
%  \centering
%  \begin{tabular}[c]{|l||r|r||r|r|}
%    \hline
%    Approximator    & Local MAE & STD   & Kaggle Pub.       & Kaggle Priv.\\ % kaggle descriptions
%    \hline
%    RF (baseline)             &           &       & 0.31661           & 0.31252\\ % With histogram fix
%    CNN             &           &       & 0.16792           & 0.18781\\ % fft_4convs_8features_MAE
%    RF (best)            &           &       & \textbf{0.09542}  & \textbf{0.10463}\\ % last one
%    \hline
%  \end{tabular}
%  \caption{Mean Absolute Error (MAE) and Standard Deviation (STD), public Kaggle score and private Kaggle score for our various similarity function approximators.}
%  \label{tab:SFA_MSS}
%\end{table}


\begin{table}[htbp]
  \centering
  \begin{tabular}[c]{|l||r|r|}
    \hline
    Approximator    & Training time     & Scoring time \\
    \hline
    SRF (baseline)             &                   &       \\
    CNN             & 24h               &       \\
    ARF             &                   &       \\
    \hline
  \end{tabular}
  \caption{Training and generation time for our various similarity function approximators.}
  \label{tab:SFA_time}
\end{table}


\subsection{Generators}

\begin{table}\centering
\begin{tabular}{lrr|rr}
\toprule
Model & RF MSS & RF STD & CNN MSS & CNN STD \\
\midrule
AG (baseline)  & 1.728 & & 1.437 & \\
large DCGAN & & & & \\
VAE & 1.723 & & 2.026 & \\
cDCGAN & \textbf{3.044} & 1.088 & \textbf{2.613} & 0.886 \\
\bottomrule
\end{tabular}
\caption{Mean similarity score (MSS) and standard deviation of the similarity score (STD) of the images generated by our models estimated by our CNN and our best random forest regression model (RF).}\label{tab:gen_MSS}
\end{table}


%\begin{table}[htbp]
%  \centering
%  \begin{tabular}[c]{|l||r|r||r|r|}
%    \hline
%    Generator   & ARF MSS       & ARF STD   & CNN MSS       & CNN STD \\
%     \hline
%     AG (baseline)          & 1.728         &           & 1.437         &  \\
%     large DCGAN          &          &           &          &  \\
%     VAE         & 1.723         &           & 2.026         &  \\
%     cDCGAN      & \textbf{3.044}& 1.088     & \textbf{2.613}& 0.886 \\
%     %PixelCNN    &               &           &               &  \\
%    \hline
%   \end{tabular}
%   \caption{Mean Similarity Score (MSS) and Standard Deviation (STD) as measured on the ARF and the CNN for our various generators. \TODO{We probably need to recalculate these}}
  \label{tab:gen_MSS}
% \end{table}




\begin{table}[htbp]
  \centering
  \begin{tabular}[c]{|l||r|r|}
    \hline
    Generator   & Training time & Generation time \\
    \hline
    AG          &  -            &       \\
    cDCGAN      &               &       \\
    VAE         &               &       \\
    %PixelCNN    &               &       \\
    \hline
  \end{tabular}
  \caption{Training and generation time for our various generators.}
  \label{tab:gen_time}
\end{table}

To evaluate our generators we used our two best similarity function approximators, i.e. our ARF and CNN, to calculate the mean similarity score of the images. We chose not to train and use a conventional classifier to evaluate the generated images, as the Kaggle competition states that the images will be scored according to their mean similarity to a prototypical cosmology image. As such we decided to use the same evaluation process.
\TODO{TODO}



\section{Discussion}
\TODO{TODO}

\section{Summary}
By selectively extracting the most important features from images and learning to generate only these we achieved generated images superior than if the generative model learns to generate the image as a whole. This is judged by our similarity scorers.
\TODO{TODO}



\section*{Acknowledgements}
We would like to thank the assistants of the Computational Intelligence Lab for all their support and invariably fast replies to all our questions.


\bibliographystyle{IEEEtran}
\bibliography{cil_report}
\end{document}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ownGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tillns/CIL_project/blob/master/Code/ownGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_f2HN8Mf4ydd",
        "colab_type": "code",
        "outputId": "f60fee5f-39ab-4f01-b08f-5a6093c737ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "use_progressive_variant = True\n",
        "\n",
        "# TODO:\n",
        "# 2) a progress bar would be kinda nice as well\n",
        "# 3) tweak params, of course\n",
        "# 4) think about batch norm/instance norm\n",
        "# \n",
        "# 6) name files based on parameters and architecture, s.t. there are individual ones for different kinds of training\n",
        "# 7) Adding noise to real input images before feeding them into the dis might help https://github.com/soumith/ganhacks/issues/14\n",
        "# 8) save model every few minutes maybe\n",
        "# 9) progressive growing proposed by Sven: https://github.com/tkarras/progressive_growing_of_gans --- Code at https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py\n",
        "#    not yet implemented use_wscale? weight decay 0.999 I think\n",
        "#    they seem to generally use float 32, maybe adjust code to that\n",
        "import os, time, itertools, pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "# for now, only those images with label 1 are kept\n",
        "# consider including also the fake ones\n",
        "\n",
        "original_image_size = 1000\n",
        "# e.g. for training on 64x64, the original images may be downsampled to 250x250\n",
        "# and during training. random 64x64 (appr. 1/4 = 1/patch_divide_factor) patches\n",
        "# are drawn from these images\n",
        "image_channels = 1\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "image_directory = os.path.join(home_dir, \"dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled\")\n",
        "label_path = os.path.join(home_dir, \"dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled.csv\")\n",
        "print(label_path)\n",
        "\n",
        "convs = []\n",
        "lrelus = []\n",
        "resblocks = []\n",
        "upsamples = []\n",
        "tf.reset_default_graph()\n",
        "tf.InteractiveSession().close()\n",
        "\n",
        "# params to tweak\n",
        "gen_num_upsample = 2\n",
        "gen_res = pow(2, gen_num_upsample)\n",
        "gen_num_resblocks = 0\n",
        "gen_num_normal_convs = 0\n",
        "dis_num_downsample = 2\n",
        "dis_res = pow(2, dis_num_downsample)\n",
        "dis_num_normal_convs = 0\n",
        "dis_num_resblocks = 0\n",
        "# my guess is that b&w star pics won't need many feature maps, bc the only \n",
        "# features are white dots and black background\n",
        "gen_start_features = 32\n",
        "dis_start_features = 16\n",
        "normal_conv_kernel_dim = 3\n",
        "normal_kernel = (normal_conv_kernel_dim, normal_conv_kernel_dim)\n",
        "downsample_conv_kernel_dim = 4\n",
        "lrelu_factor = 0.2\n",
        "batch_size = 32\n",
        "lr = 0.0002\n",
        "train_epoch = 5\n",
        "GAN_loss_type = 'LSGAN'\n",
        "# defined in load_dataset\n",
        "dataset_length = 0\n",
        "num_iterations = 0\n",
        "downsample_size = 0\n",
        "\n",
        "# some params for progressive growing GAN\n",
        "max_features = 64\n",
        "min_features = 2\n",
        "start_res = 4\n",
        "# think about difference to 1000 (possibility is a conv that reduces 128 to 125,\n",
        "# which would require valid paddding with either a kernel size of 4x4 or \n",
        "# removing one line to achieve a 127x127 spatial res. Not sure which would be worse ^^)\n",
        "end_res = original_image_size  \n",
        "num_res_change = 9\n",
        "num_const_feature_layers = 4\n",
        "mbstd_group_size = 4\n",
        "# cross faded to and from rgb layers (first in gen, last in dis)\n",
        "to_rgb = []\n",
        "gen_convs = []\n",
        "gen_dense = []\n",
        "from_rgb = []\n",
        "dis_convs = []\n",
        "dis_dense = []\n",
        "dis_pool = tf.keras.layers.AveragePooling2D()\n",
        "combined_res_change_and_conv = False  # can't be true yet\n",
        "\n",
        "\n",
        "def load_dataset(image_size=64, patch_divide_factor=4):\n",
        "  factor_to_downsample = max(1024//image_size//patch_divide_factor, 1)\n",
        "  global downsample_size\n",
        "  downsample_size = original_image_size//factor_to_downsample  \n",
        "  print(\"Downsampling input images to: {}x{}\".format(downsample_size, downsample_size))\n",
        "  try:\n",
        "    f=open(label_path,'r')\n",
        "    label_list = []\n",
        "    for line in f:\n",
        "      if not \"Id,Actual\" in line:\n",
        "        label_list.append(line.split(\",\"))\n",
        "    label_list = sorted(label_list)\n",
        "    labels = np.zeros(len(label_list))\n",
        "    for ind, label in enumerate(label_list):\n",
        "      labels[ind] = label[1]\n",
        "\n",
        "    imgs = np.empty((0, downsample_size, downsample_size, image_channels)) \n",
        "    img_list = []\n",
        "    for filename in os.listdir(image_directory):\n",
        "      if filename.endswith(\".png\") and not filename.startswith(\"._\"):\n",
        "         img_list.append(filename)\n",
        "\n",
        "\n",
        "    img_list = sorted(img_list)\n",
        "    for ind, filename in enumerate(img_list):\n",
        "      if labels[ind] == 1:\n",
        "        img = Image.open(os.path.join(image_directory, filename)).resize((downsample_size, downsample_size))\n",
        "        imgs = np.append(imgs, np.array(img).reshape((1, downsample_size, downsample_size, image_channels)), axis=0)\n",
        "    # imgs = (imgs - 0.5) / 0.5  # if they go from 0 to 1, use this to make them go from -1 to 1\n",
        "\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(\"Dataset not found. Using dummy dataset\")\n",
        "    labels = np.ones(1000)\n",
        "    imgs = np.random.normal(0, 1, (1000, downsample_size, downsample_size, image_channels))\n",
        "  \n",
        "\n",
        "  global num_iterations\n",
        "  global dataset_length\n",
        "  dataset_length = int(imgs.shape[0])\n",
        "  num_iterations = dataset_length // batch_size\n",
        "  # this could get useful if we include the fake images from the dataset with label 0 \n",
        "  # labels_wide = np.zeros((dataset_length, image_size//dis_res, image_size//dis_res, 1))\n",
        "  # for ind in range(dataset_length):\n",
        "  #  labels_wide[ind, :, :, :] = labels[ind]\n",
        "  return imgs\n",
        "\n",
        "  \n",
        "def build_progr_gen_layers(transp=False):\n",
        "  with tf.variable_scope('generator'):\n",
        "    features = max_features\n",
        "    # first special block\n",
        "    gen_dense.append(tf.keras.layers.Dense(features*start_res*start_res))\n",
        "    gen_convs.append(tf.keras.layers.Conv2D(features, normal_kernel, padding='same'))\n",
        "    to_rgb.append(tf.keras.layers.Conv2D(image_channels, kernel_size=(1, 1), padding='same'))\n",
        "\n",
        "    # rest normal blocks\n",
        "    for num_upsample in range(num_res_change):\n",
        "      if num_upsample >= num_const_feature_layers-1: \n",
        "        features = features//2\n",
        "\n",
        "      if transp:\n",
        "        # Put transposed conv here. Should be more efficient but could also produce checkerboards, we'll see\n",
        "        pass\n",
        "      else:\n",
        "        gen_convs.append(tf.keras.layers.Conv2D(features, normal_kernel, padding='same'))\n",
        "      if start_res*pow(2, num_upsample) == 128:\n",
        "        # this should decrease the spatial resolution from 128 to 125 to produce 1000x1000 images in the end\n",
        "        gen_convs.append(tf.keras.layers.Conv2D(features, kernel_size=(4, 4), padding='valid'))\n",
        "      else:\n",
        "        gen_convs.append(tf.keras.layers.Conv2D(features, normal_kernel, padding='same'))\n",
        "      # every block is followed by a to_rgb conv that are faded into one another\n",
        "      to_rgb.append(tf.keras.layers.Conv2D(image_channels, kernel_size=(1, 1), padding='same'))\n",
        "      \n",
        "def build_progr_dis_layers(strided_conv=False):\n",
        "  with tf.variable_scope('discriminator'):\n",
        "    features = min_features\n",
        "\n",
        "    # downsampling convs (reduce resolution, increase features)\n",
        "    for num_downsample in range(num_res_change):\n",
        "      # this layer has to be cross faded for res changes\n",
        "      from_rgb.append(tf.keras.layers.Conv2D(features, kernel_size=(1, 1), padding='same'))\n",
        "      print(from_rgb[-1].get_config())\n",
        "\n",
        "      if end_res//pow(2, num_downsample) == 125:\n",
        "        # padding: 125x125 -> 131x131, then valid padding conv 4x4: 131x131 -> 128x128. CHECK\n",
        "        dis_convs.append(tf.keras.layers.Conv2D(features, kernel_size=(4, 4), padding='valid'))\n",
        "        print(dis_convs[-1].get_config())\n",
        "      else:\n",
        "        dis_convs.append(tf.keras.layers.Conv2D(features, normal_kernel, padding='same'))\n",
        "        print(dis_convs[-1].get_config())\n",
        "      if num_downsample <= num_res_change-num_const_feature_layers-1:\n",
        "        features=features*2\n",
        "      if strided_conv:\n",
        "        # add strided conv\n",
        "        pass\n",
        "      else:\n",
        "        dis_convs.append(tf.keras.layers.Conv2D(features, normal_kernel, padding='same'))\n",
        "        print(dis_convs[-1].get_config())\n",
        "\n",
        "    from_rgb.append(tf.keras.layers.Conv2D(features, kernel_size=(1, 1), padding='same'))\n",
        "    print(from_rgb[-1].get_config())\n",
        "    dis_convs.append(tf.keras.layers.Conv2D(features, normal_kernel, strides=(1, 1), padding='same'))\n",
        "    print(dis_convs[-1].get_config())\n",
        "    dis_dense.append(tf.keras.layers.Dense(features))\n",
        "    print(dis_dense[-1].get_config())\n",
        "    dis_dense.append(tf.keras.layers.Dense(1))\n",
        "    print(dis_dense[-1].get_config())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OgUOcUh43yuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from random import randint\n",
        "\n",
        "def lrelu(x, th=0.2):\n",
        "    return tf.maximum(th * x, x)\n",
        "  \n",
        "def pixel_norm(x, epsilon=1e-8):\n",
        "  return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + epsilon)\n",
        "\n",
        "\n",
        "# nearest neighbor upscaling. copied from progressive GAN paper; adjusted bc dimensions were ordered differently.\n",
        "def upscale2d(x, factor=2):\n",
        "    assert isinstance(factor, int) and factor >= 1\n",
        "    if factor == 1: return x\n",
        "    with tf.variable_scope('Upscale2D'):\n",
        "        s = x.shape\n",
        "        x = tf.reshape(x, [-1, s[1], 1, s[2], 1, s[3]])\n",
        "        x = tf.tile(x, [1, 1, factor, 1, factor, 1])\n",
        "        x = tf.reshape(x, [-1, s[1] * factor, s[2] * factor, s[3]])\n",
        "        return x\n",
        "\n",
        "# Minibatch standard deviation.\n",
        "\n",
        "def minibatch_stddev_layer(x, group_size=4):\n",
        "    with tf.variable_scope('MinibatchStddev'):\n",
        "        #group_size = tf.minimum(group_size, tf.shape(x)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "        s = x.get_shape().as_list()                                            # [NCHW]  Input shape.\n",
        "        print(s)\n",
        "        group_size = min(group_size, batch_size)\n",
        "        y = tf.reshape(x, (group_size, -1, s[1], s[2], s[3]))   # [GMCHW] Split minibatch into M groups of size G.\n",
        "        y = tf.cast(y, tf.float32)                              # [GMCHW] Cast to FP32.\n",
        "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMCHW] Subtract mean over group.\n",
        "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MCHW]  Calc variance over group.\n",
        "        y = tf.sqrt(y + 1e-8)                                   # [MCHW]  Calc stddev over group.\n",
        "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
        "        y = tf.cast(y, x.dtype)                                 # [M111]  Cast back to original data type.\n",
        "        y = tf.tile(y, [group_size, s[1], s[2], 1])             # [N1HW]  Replicate over group and pixels.\n",
        "        return tf.concat([x, y], axis=3)                        # [NCHW]  Append as new fmap.\n",
        "\n",
        "      \n",
        "# in the progression paper, they apply the activation (lrelu) BEFORE the pixel normalization\n",
        "def activated_norm(x, lrelu_factor=0.2, isTrain=True, normalization='pixel'):\n",
        "  if normalization == 'pixel':\n",
        "    return pixel_norm(lrelu(x, lrelu_factor))\n",
        "  elif normalization == 'batch':\n",
        "    return lrelu(tf.layers.batch_normalization(x, training=isTrain), lrelu_factor)\n",
        "  else:\n",
        "    return lrelu(x, lrelu_factor)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "# number of input features MUST equal number of output features!\n",
        "def get_resblock(x, features=256, kernel=[3, 3], strides=(1, 1), padding='same', lrelu_factor=0.2, isTrain=True):\n",
        "    convs.append(tf.layers.conv2d(x, features, kernel, strides, padding))\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "    convs.append(tf.layers.conv2d(lrelus[-1], features, kernel, strides, padding))\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "    return lrelus[-1]+x;\n",
        "  \n",
        "  \n",
        "# gen and dis after growing GANs paper. In the paper, they describe their up-/downsampling\n",
        "# methods to be separate from the conv while in the code, they also explore the combination\n",
        "# with transposed/strided convolution. Maybe add this here, it's more efficient.  \n",
        "\n",
        "# TODO: The rough architecture should be right now. Run and fix all errors.\n",
        "# Also, there is no implementation for the real progression part yet. For this, \n",
        "# some layers would probably have to load the saved weights from the training with \n",
        "# fewer layers. Not exactly sure how to do it yet.\n",
        "def generator_progr(x, isTrain=True, reuse=False, transp=False, num_upsample_blocks=0, \n",
        "                    cross_fade_alpha=1): # 1 for no cross fading. \n",
        "  with tf.variable_scope('generator', reuse=reuse):\n",
        "    # first conv block\n",
        "    x = tf.reshape(x, (-1, max_features))  # this assumes the input to have 1x1 spatial res\n",
        "    x = gen_dense[0](x)\n",
        "    x = tf.reshape(x, (-1, start_res, start_res, max_features))\n",
        "    x = activated_norm(x, lrelu_factor, isTrain, normalization='pixel')\n",
        "    x = gen_convs[0](x)\n",
        "    x = activated_norm(x, lrelu_factor, isTrain, normalization='pixel')\n",
        "    alpha = tf.reshape(cross_fade_alpha, (1, 1, 1, 1))  \n",
        "    if num_upsample_blocks == 0 or cross_fade_alpha == 1:\n",
        "      add_x = 0\n",
        "    for num_upsample in range(num_upsample_blocks):\n",
        "\n",
        "      if not transp:\n",
        "        x = upscale2d(x)\n",
        "      if num_upsample == num_upsample_blocks-1 and cross_fade_alpha != 1:  \n",
        "        add_x = (1-alpha)*to_rgb[num_upsample_blocks-1](x)  # might have to replace it if transp conv is used bc the resolution would be wrong, I think\n",
        "\n",
        "      x = gen_convs[2*num_upsample+1](x)\n",
        "      x = activated_norm(x, lrelu_factor, isTrain, normalization='pixel')\n",
        "      x = gen_convs[2*num_upsample+2](x)\n",
        "      x = activated_norm(x, lrelu_factor, isTrain, normalization='pixel')\n",
        "\n",
        "    # apparently, scalar multiplication is problematic for the dynamic shape guess of tf;\n",
        "    # I don't know why it works for add_x tho\n",
        "\n",
        "    x = alpha*to_rgb[num_upsample_blocks](x) + add_x\n",
        "\n",
        "    return x\n",
        "\n",
        "    \n",
        "# D(x)\n",
        "def discriminator_progr(x, isTrain=True, reuse=False, strided_conv=False, num_downsample_blocks=0,\n",
        "                        cross_fade_alpha=1):\n",
        "  with tf.variable_scope('discriminator'):\n",
        "    # this layer has to be cross faded for res changes\n",
        "    add_x = x\n",
        "    alpha = tf.reshape(cross_fade_alpha, (1, 1, 1, 1))\n",
        "    x = alpha*from_rgb[-1-num_downsample_blocks](x)\n",
        "    x = activated_norm(x, lrelu_factor, isTrain, normalization='None')\n",
        "\n",
        "    # downsampling convs (reduce resolution, increase features)\n",
        "    for num_downsample in range(num_downsample_blocks):\n",
        "      if end_res//pow(2, num_downsample) == 125:\n",
        "        # padding: 125x125 -> 131x131, then valid padding conv 4x4: 131x131 -> 128x128. CHECK\n",
        "        x = tf.pad(x, tf.constant([[0, 0], [3, 3], [3, 3], [0, 0]])) \n",
        "      x = dis_convs[-3-2*num_downsample](x)\n",
        "      x = activated_norm(x, lrelu_factor, isTrain, normalization='None')\n",
        "      x = dis_convs[-2-2*num_downsample](x)\n",
        "      x = activated_norm(x, lrelu_factor, isTrain, normalization='None')\n",
        "      if not strided_conv:\n",
        "        x = dis_pool(x)\n",
        "        if num_downsample==0 and cross_fade_alpha!=1:\n",
        "          add_x = dis_pool(add_x)\n",
        "          x += (1-alpha)*from_rgb[-2-num_downsample_blocks](add_x)\n",
        "\n",
        "    if mbstd_group_size > 1:\n",
        "      x = minibatch_stddev_layer(x, mbstd_group_size)\n",
        "    x = dis_convs[-1](x)\n",
        "    x = tf.reshape(x, (-1, max_features*start_res*start_res))  # assumes input to be features x start_res x start_res\n",
        "    x = dis_dense[0](x)\n",
        "    x = activated_norm(x, lrelu_factor, isTrain, normalization='None')\n",
        "    x = dis_dense[1](x)\n",
        "\n",
        "    return x\n",
        "  \n",
        "      \n",
        "    \n",
        "# G(z)\n",
        "def generator(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "      \n",
        "      features = gen_start_features\n",
        "      # resblocks (num features kept constant)\n",
        "      for num_resblock in range(gen_num_resblocks):\n",
        "          if num_resblock == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = resblocks[-1]\n",
        "          resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # upsample + conv (num features reduced)\n",
        "      for num_upsample in range(gen_num_upsample):\n",
        "        if num_upsample == 0:\n",
        "          if gen_num_resblocks == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = resblocks[-1]\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        print(\"gen 1 shape: {}\".format(input_im.shape))\n",
        "        upsamples.append(upscale2d(input_im))\n",
        "        convs.append(tf.layers.conv2d(upsamples[-1], features, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same'))\n",
        "        print(\"Len convs: {}\".format(len(convs)))\n",
        "        # maybe change to instance normalization (tf.contrib.layers.instance_norm(input, trainable)) \n",
        "        # this does not exist in tf 1.3 yet and I haven't gotten it to work on 1.13 either\n",
        "        # error probably lies in trainable, haven't understood that yet\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features//2\n",
        "\n",
        "      # output layer\n",
        "      if gen_num_upsample == 0:\n",
        "        if gen_num_resblocks == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "      else:\n",
        "        input_im = lrelus[-1]\n",
        "      print(\"gen 2 shape: {}\".format(input_im.shape))\n",
        "      conv5 = tf.layers.conv2d(input_im, image_channels, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      # never normalize output\n",
        "      o = tf.nn.tanh(conv5)\n",
        "\n",
        "      return o\n",
        "\n",
        "# D(x)\n",
        "def discriminator(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "      \n",
        "      features = dis_start_features\n",
        "      # downsampling convs (reduce resolution, increse features)\n",
        "      for num_downsample in range(dis_num_downsample):\n",
        "        if num_downsample == 0:\n",
        "            input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        convs.append(tf.layers.conv2d(input_im, features, [downsample_conv_kernel_dim, downsample_conv_kernel_dim], strides=(2, 2), padding='same'))\n",
        "        # maybe change to instance normalization\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features*2\n",
        "        \n",
        "      # resblocks (features kept constant)  \n",
        "      for num_resblock in range(dis_num_resblocks):\n",
        "        if num_resblock == 0:\n",
        "          if dis_num_downsample == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = lrelus[-1]\n",
        "            features=features//2\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "        resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # output layer without fc, s.t. local patches are rated and generator is\n",
        "      # forced to be cinsistent everywhere (PatchGAN). Could be changed tho...\n",
        "      if dis_num_resblocks == 0:\n",
        "        if dis_num_downsample == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "      else:\n",
        "        input_im = resblocks[-1]\n",
        "      conv6 = tf.layers.conv2d(input_im, 1, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      #fc = tf.layers.dense(conv6, 32*32, activation='sigmoid')\n",
        "      o = tf.nn.sigmoid(conv6)\n",
        "      print(\"D output shape: {}\".format(o.shape))\n",
        "      return o\n",
        "    \n",
        "def D(x, isTrain=True, reuse=False, strided_conv=False, num_downsample_blocks=0,\n",
        "                        cross_fade_alpha=1):\n",
        "  if use_progressive_variant:\n",
        "    return discriminator_progr(x, isTrain, reuse, strided_conv, \n",
        "                               num_downsample_blocks, cross_fade_alpha)\n",
        "  else:\n",
        "    return discriminator(x, isTrain, reuse)\n",
        "  \n",
        "def G(x, isTrain=True, reuse=False, transp=False, num_upsample_blocks=0,\n",
        "                        cross_fade_alpha=1):\n",
        "  if use_progressive_variant:\n",
        "    return generator_progr(x, isTrain, reuse, transp, num_upsample_blocks,\n",
        "                           cross_fade_alpha)\n",
        "  else:\n",
        "    return generator(x, isTrain, reuse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tgIt_tmMc5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the WGAN-GP loss for the generator and discriminator, copied from the growing GAN paper. "
      ]
    },
    {
      "metadata": {
        "id": "sp4PP9qqMplf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# linearly interpolate between a and b\n",
        "def lerp(a, b, t):\n",
        "    with tf.name_scope('Lerp'):\n",
        "        return a + (b - a) * t\n",
        "      \n",
        "def exp2(x):\n",
        "    with tf.name_scope('Exp2'):\n",
        "        return tf.exp(x * np.float32(np.log(2.0)))\n",
        "      \n",
        "def apply_loss_scaling(value, use_loss_scaling=False):\n",
        "    assert is_tf_expression(value)\n",
        "    if not use_loss_scaling:\n",
        "        return value\n",
        "    return value * exp2(value)\n",
        "\n",
        "# Undo the effect of dynamic loss scaling for the given expression.\n",
        "def undo_loss_scaling(value):\n",
        "    assert is_tf_expression(value, use_loss_scaling=False)\n",
        "    if not use_loss_scaling:\n",
        "        return value\n",
        "    return value * exp2(-value)\n",
        "      \n",
        "def autosummary(name, value):\n",
        "    id = name.replace('/', '_')\n",
        "    if is_tf_expression(value):\n",
        "        with tf.name_scope('summary_' + id), tf.device(value.device):\n",
        "            update_op = _create_autosummary_var(name, value)\n",
        "            with tf.control_dependencies([update_op]):\n",
        "                return tf.identity(value)\n",
        "    else: # python scalar or numpy array\n",
        "        if name not in _autosummary_immediate:\n",
        "            with absolute_name_scope('Autosummary/' + id), tf.device(None), tf.control_dependencies(None):\n",
        "                update_value = tf.placeholder(tf.float32)\n",
        "                update_op = _create_autosummary_var(name, update_value)\n",
        "                _autosummary_immediate[name] = update_op, update_value\n",
        "        update_op, update_value = _autosummary_immediate[name]\n",
        "        run(update_op, {update_value: np.float32(value)})\n",
        "        return value\n",
        "      \n",
        "def fp32(*values):\n",
        "    if len(values) == 1 and isinstance(values[0], tuple):\n",
        "        values = values[0]\n",
        "    values = tuple(tf.cast(v, tf.float32) for v in values)\n",
        "    return values if len(values) >= 2 else values[0]\n",
        "\n",
        "# wgangp losses. I crossed out the label penalty terms. I think those were introduced in\n",
        "# https://arxiv.org/pdf/1610.09585.pdf the ac-gan paper. For multi-class generation,\n",
        "# this term is useful, so it's probably not necessary in this work\n",
        "def G_wgangp(labels, fake_scores_out,\n",
        "    cond_weight = 1.0): # Weight of the conditioning term.\n",
        "\n",
        "    #latents = tf.random_normal([batch_size][image_size])  \n",
        "    #labels = training_set.get_random_labels_tf(minibatch_size)\n",
        "    #fake_images_out = G(latents, labels, is_training=True)\n",
        "    #fake_scores_out, fake_labels_out = fp32(D(fake_images_out, is_training=True))\n",
        "    loss = -fake_scores_out\n",
        "\n",
        "    # crossed this out\n",
        "    #if D.output_shapes[1][1] > 0: \n",
        "    #with tf.name_scope('LabelPenalty'):\n",
        "    #    label_penalty_fakes = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fake_labels_out)\n",
        "    #loss += label_penalty_fakes * cond_weight\n",
        "    return loss\n",
        "\n",
        "\n",
        "def D_wgangp(minibatch_size, reals, fake_images_out, real_scores_out, fake_scores_out,\n",
        "    wgan_lambda     = 10.0,     # Weight for the gradient penalty term.\n",
        "    wgan_epsilon    = 0.001,    # Weight for the epsilon term, \\epsilon_{drift}.\n",
        "    wgan_target     = 1.0,      # Target value for gradient magnitudes.\n",
        "    cond_weight     = 1.0):     # Weight of the conditioning terms.\n",
        "\n",
        "    #latents = tf.random_normal([minibatch_size] + G.input_shapes[0][1:])\n",
        "    #fake_images_out = G.get_output_for(latents, labels, is_training=True)\n",
        "    #real_scores_out, real_labels_out = fp32(D.get_output_for(reals, is_training=True))\n",
        "    #fake_scores_out, fake_labels_out = fp32(D.get_output_for(fake_images_out, is_training=True))\n",
        "    real_scores_out = tfutil.autosummary('Loss/real_scores', real_scores_out)\n",
        "    fake_scores_out = tfutil.autosummary('Loss/fake_scores', fake_scores_out)\n",
        "    loss = fake_scores_out - real_scores_out  # this is the normale WGAN loss\n",
        "\n",
        "    with tf.name_scope('GradientPenalty'):\n",
        "        mixing_factors = tf.random_uniform([minibatch_size, 1, 1, 1], 0.0, 1.0, dtype=fake_images_out.dtype)\n",
        "        mixed_images_out = tfutil.lerp(tf.cast(reals, fake_images_out.dtype), fake_images_out, mixing_factors)\n",
        "        mixed_scores_out, mixed_labels_out = fp32(D(mixed_images_out, is_training=True))\n",
        "        mixed_scores_out = tfutil.autosummary('Loss/mixed_scores', mixed_scores_out)\n",
        "        mixed_loss = apply_loss_scaling(tf.reduce_sum(mixed_scores_out))\n",
        "        mixed_grads = undo_loss_scaling(fp32(tf.gradients(mixed_loss, [mixed_images_out])[0]))\n",
        "        mixed_norms = tf.sqrt(tf.reduce_sum(tf.square(mixed_grads), axis=[1,2,3]))\n",
        "        mixed_norms = tfutil.autosummary('Loss/mixed_norms', mixed_norms)\n",
        "        gradient_penalty = tf.square(mixed_norms - wgan_target)\n",
        "    loss += gradient_penalty * (wgan_lambda / (wgan_target**2))  # this is the gradient penalty term\n",
        "\n",
        "    with tf.name_scope('EpsilonPenalty'):\n",
        "        epsilon_penalty = tfutil.autosummary('Loss/epsilon_penalty', tf.square(real_scores_out))\n",
        "    loss += epsilon_penalty * wgan_epsilon  # this is the small additional loss mentioned to avoid drifts from zero\n",
        "\n",
        "    #if D.output_shapes[1][1] > 0:\n",
        "    #with tf.name_scope('LabelPenalty'):\n",
        "    #    label_penalty_reals = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=real_labels_out)\n",
        "    #    label_penalty_fakes = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fake_labels_out)\n",
        "    #    label_penalty_reals = tfutil.autosummary('Loss/label_penalty_reals', label_penalty_reals)\n",
        "    #    label_penalty_fakes = tfutil.autosummary('Loss/label_penalty_fakes', label_penalty_fakes)\n",
        "    #loss += (label_penalty_reals + label_penalty_fakes) * cond_weight\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_W3y6vvSQGN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_result(num_epoch, sess, G_z, z, isTrain, cross_fade_alpha, alpha_=1, show = False, save = False, path = 'result.png', image_size=64):\n",
        "  size_figure_grid = 5\n",
        "  if use_progressive_variant:\n",
        "    spatial = 1\n",
        "    num_features = max_features\n",
        "  else:\n",
        "    spatial = image_size//gen_res\n",
        "    num_features = gen_start_features\n",
        "  fixed_z_ = np.random.normal(0, 1, (size_figure_grid*size_figure_grid, spatial, spatial, num_features))\n",
        "  test_images = sess.run(G_z, {z: fixed_z_, isTrain: False, cross_fade_alpha: alpha_})\n",
        "\n",
        "  fig_size = image_size*size_figure_grid*1.3/100  # appr. formula s.t. image resolutions aren't reduced\n",
        "  fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(fig_size, fig_size))\n",
        "  for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "      ax[i, j].get_xaxis().set_visible(False)\n",
        "      ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "  for k in range(size_figure_grid*size_figure_grid):\n",
        "      i = k // size_figure_grid\n",
        "      j = k % size_figure_grid\n",
        "      ax[i, j].cla()\n",
        "      ax[i, j].imshow(np.reshape(test_images[k], (image_size, image_size)), cmap='gray')\n",
        "\n",
        "  label = 'Epoch {0}'.format(num_epoch)\n",
        "  fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "  if save:\n",
        "      plt.savefig(path)\n",
        "\n",
        "  if show:\n",
        "      plt.show()\n",
        "  else:\n",
        "      plt.close()\n",
        "\n",
        "# plots graph with dis and gen loss \n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "#with tf.device('/cpu:0'):\n",
        "def train(imgs, num_res_change_layers=0, cross_fade=False, device='/device:GPU:0', image_size=64):\n",
        "  with tf.device(device): \n",
        "    global gen_res\n",
        "    global gen_start_features\n",
        "    if use_progressive_variant:\n",
        "      gen_res = image_size\n",
        "      gen_start_features = max_features\n",
        "    # variables : input\n",
        "    x = tf.placeholder(tf.float32, shape=(None, image_size, image_size, image_channels))\n",
        "    z = tf.placeholder(tf.float32, shape=(None, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "    cross_fade_alpha = tf.placeholder(dtype=tf.float32)\n",
        "    isTrain = tf.placeholder(dtype=tf.bool)\n",
        "    \n",
        "    \n",
        "\n",
        "    # networks : generator\n",
        "    G_z = G(z, isTrain, transp=combined_res_change_and_conv, num_upsample_blocks=num_res_change_layers, cross_fade_alpha=cross_fade_alpha)\n",
        "\n",
        "    # networks : discriminator\n",
        "    D_real_logits = D(x, isTrain, strided_conv=combined_res_change_and_conv, num_downsample_blocks=num_res_change_layers, cross_fade_alpha=cross_fade_alpha)\n",
        "    D_fake_logits = D(G_z, isTrain, reuse=True, strided_conv=combined_res_change_and_conv, num_downsample_blocks=num_res_change_layers, cross_fade_alpha=cross_fade_alpha)\n",
        "\n",
        "    # this assumes that the progressive variant has an fc at the end of the dis and the other one does not\n",
        "    if use_progressive_variant: \n",
        "      label_zero = tf.zeros([batch_size, 1])\n",
        "      label_one = tf.ones([batch_size, 1])\n",
        "    else:\n",
        "      label_zero = tf.zeros([batch_size, image_size//dis_res, image_size//dis_res, 1])\n",
        "      label_one = tf.ones([batch_size, image_size//dis_res, image_size//dis_res, 1])\n",
        "      \n",
        "    # loss for each network\n",
        "    # think about GAN objective\n",
        "    if GAN_loss_type == 'LSGAN':\n",
        "      print(\"Using LSGAN. Might still be wrong.\")\n",
        "      D_loss = tf.reduce_mean(tf.square(D_fake_logits - label_zero)) + tf.reduce_mean(tf.square(D_real_logits - label_one))\n",
        "      G_loss = tf.reduce_mean(tf.square(D_fake_logits - label_one))\n",
        "    elif GAN_loss_type == 'WGAN-GP':\n",
        "      print(\"Using WGAN-GP without label penalty.\")\n",
        "      G_loss = G_wgangp(label_one, D_fake_logits)\n",
        "      D_loss = D_wgangp(batch_size, x, G(z), D_real_logits, D_fake_logits)\n",
        "    else:\n",
        "      print(\"Using the GAN objective that was already implemented, not sure which it is\")\n",
        "      D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=label_one))\n",
        "      D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=label_zero))\n",
        "      D_loss = D_loss_real + D_loss_fake\n",
        "      G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=label_one))\n",
        "\n",
        "    # trainable variables for each network\n",
        "    T_vars = tf.trainable_variables()\n",
        "    D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
        "    G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
        "\n",
        "    # optimizer for each network\n",
        "    # This should probably not be experimented with. ADAM is standard nowadays.\n",
        "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):      \n",
        "      D_optimizer = tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "      G_optimizer = tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "\n",
        "      # access the gradients - HANNES\n",
        "      D_grads = D_optimizer.compute_gradients(D_loss, var_list=D_vars)\n",
        "      G_grads = G_optimizer.compute_gradients(G_loss, var_list=G_vars)\n",
        "\n",
        "      D_optim = D_optimizer.apply_gradients(D_grads)\n",
        "      G_optim = G_optimizer.apply_gradients(G_grads)\n",
        "\n",
        "\n",
        "    # create loss summaries for tensorboard - HANNES\n",
        "    D_loss_summary = tf.summary.scalar(name=\"D_Loss\", tensor=D_loss)\n",
        "\n",
        "    G_loss_summary = tf.summary.scalar(name=\"G_loss\", tensor=G_loss)\n",
        "\n",
        "    # create weight summaries for tensorboard - HANNES\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(\"generator\", reuse=True):\n",
        "      with tf.variable_scope(\"conv2d\", reuse=True): # feature reduction\n",
        "        g_conv2d_weights_summary = tf.summary.histogram(name=\"g_conv2d_weights\", values=tf.get_variable(\"kernel\"))\n",
        "      with tf.variable_scope(\"conv2d_1\", reuse=True): # feature reduction\n",
        "        g_conv2d_1_weights_summary = tf.summary.histogram(name=\"g_conv2d_1_weights\", values=tf.get_variable(\"kernel\"))\n",
        "      with tf.variable_scope(\"conv2d_2\", reuse=True): # output\n",
        "        g_conv2d_2_weights_summary = tf.summary.histogram(name=\"g_conv2d_2_weights\", values=tf.get_variable(\"kernel\"))\n",
        "\n",
        "    with tf.variable_scope(\"discriminator\", reuse=True):\n",
        "      with tf.variable_scope(\"conv2d\", reuse=True): # feature increase\n",
        "        d_conv2d_weights_summary = tf.summary.histogram(name=\"d_conv2d_weights\", values=tf.get_variable(\"kernel\"))\n",
        "      with tf.variable_scope(\"conv2d_1\", reuse=True): # feature increase\n",
        "        d_conv2d_1_weights_summary = tf.summary.histogram(name=\"d_conv2d_1_weights\", values=tf.get_variable(\"kernel\"))\n",
        "      with tf.variable_scope(\"conv2d_2\", reuse=True): # output\n",
        "        d_conv2d_2_weights_summary = tf.summary.histogram(name=\"d_conv2d_2_weights\", values=tf.get_variable(\"kernel\"))\n",
        "    \"\"\"\n",
        "    if cross_fade_alpha != 1:\n",
        "      for grad in D_grads:\n",
        "        print(grad)\n",
        "\n",
        "    # create gradient summaries for tensorboard - HANNES\n",
        "    for index, grad in enumerate(D_grads):\n",
        "      tf.summary.histogram(\"{}_grad\".format(D_grads[index][1].name.replace(\":\", \"_\")), D_grads[index])\n",
        "    for index, grad in enumerate(G_grads):\n",
        "      tf.summary.histogram(\"{}grad\".format(G_grads[index][1].name.replace(\":\", \"_\")), G_grads[index])\n",
        "\n",
        "    # merge summaries - HANNES\n",
        "    all_summaries = tf.summary.merge_all() \n",
        "\n",
        "\n",
        "    # open session and initialize all variables\n",
        "    sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "    # visualize graph in tensorboard - HANNES\n",
        "    fileWriter = tf.summary.FileWriter(\"./ownGan_logdir\", tf.get_default_graph())\n",
        "\n",
        "\n",
        "    # resize and normalization\n",
        "    # train_set = dataset#.eval()\n",
        "    #train_set = (train_set - 0.5) / 0.5  # normalization; range: -1 ~ 1\n",
        "\n",
        "    # results save folder\n",
        "    root = 'DCGAN_results/'\n",
        "    model = 'DCGAN_'\n",
        "    if not os.path.isdir(root):\n",
        "        os.mkdir(root)\n",
        "    if not os.path.isdir(root + 'Fixed_results'):\n",
        "        os.mkdir(root + 'Fixed_results')\n",
        "\n",
        "    train_hist = {}\n",
        "    train_hist['D_losses'] = []\n",
        "    train_hist['G_losses'] = []\n",
        "    train_hist['per_epoch_ptimes'] = []\n",
        "    train_hist['total_ptime'] = []\n",
        "\n",
        "    # training-loop\n",
        "    np.random.seed(int(time.time()))\n",
        "    print('training start!')\n",
        "    start_time = time.time()\n",
        "    for epoch in range(train_epoch):\n",
        "      G_losses = []\n",
        "      D_losses = []\n",
        "      epoch_start_time = time.time()\n",
        "      alpha_ = 1\n",
        "      #dataset.shuffle(buffer_size=dataset_length)\n",
        "      #dataset = dataset.batch(batch_size)\n",
        "      #iterator = dataset.make_one_shot_iterator()\n",
        "\n",
        "      # TODO: include progress bar\n",
        "      # to avoid overfitting, the images are shuffled every epoch\n",
        "      # if fake images are included, shuffle an index list to access the corresponding label along with the image\n",
        "      np.random.shuffle(imgs)\n",
        "      for iteration in range(num_iterations):\n",
        "        #x_ = iterator.get_next()\n",
        "        #x_ = x_.eval()\n",
        "\n",
        "        # update discriminator\n",
        "        x_ = imgs[iteration*batch_size:(iteration+1)*batch_size]\n",
        "        random_startx = randint(0, downsample_size-image_size)\n",
        "        random_starty = randint(0, downsample_size-image_size)\n",
        "        # draw a random patch from the input\n",
        "        x_ = x_[:, random_startx:random_startx+image_size, random_starty:random_starty+image_size]\n",
        "        for i in range(2):\n",
        "          if randint(0, 1):\n",
        "            x_ = np.flip(x_, axis=i+1)  # randomly flip x- and y-axis\n",
        "\n",
        "\n",
        "        z_ = np.random.normal(0, 1, (batch_size, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "        if cross_fade:\n",
        "          alpha_ = (epoch*num_iterations+iteration)/(train_epoch*num_iterations)  # should gradually go from 0 to 1. CHECK\n",
        "          print(\"Current alpha: {}\".format(alpha_))\n",
        "\n",
        "        # just do one single sess.run() - HANNES\n",
        "        summary, loss_d_, _, loss_g_, _ = sess.run([all_summaries, D_loss, D_optim, G_loss, G_optim], {x: x_, z: z_, isTrain: True, cross_fade_alpha: alpha_})\n",
        "        D_losses.append(loss_d_)\n",
        "        G_losses.append(loss_g_)\n",
        "\n",
        "        # save one data point per epoch for tensorboard - HANNES\n",
        "        if(iteration == dataset_length // batch_size - 1):\n",
        "          fileWriter.add_summary(summary, epoch)\n",
        "\n",
        "\n",
        "    #       loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, isTrain: True})\n",
        "    #       D_losses.append(loss_d_)\n",
        "\n",
        "    #       # update generator\n",
        "    #       z_ = np.random.normal(0, 1, (batch_size, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "    #       loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, isTrain: True})\n",
        "    #       G_losses.append(loss_g_)\n",
        "\n",
        "      epoch_end_time = time.time()\n",
        "      per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "      print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n",
        "      fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n",
        "      show_result((epoch + 1), sess, G_z, z, isTrain, cross_fade_alpha, alpha_, save=True, show=False, path=fixed_p, image_size=image_size)\n",
        "      train_hist['D_losses'].append(np.mean(D_losses))\n",
        "      train_hist['G_losses'].append(np.mean(G_losses))\n",
        "      train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_ptime = end_time - start_time\n",
        "    train_hist['total_ptime'].append(total_ptime)\n",
        "\n",
        "    print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n",
        "    print(\"Training finish!... save training results\")\n",
        "    with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
        "        pickle.dump(train_hist, f)\n",
        "\n",
        "    show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
        "\n",
        "    # this thing makes a gif out of the results after every epoch, so it's not\n",
        "    # necessary \n",
        "    try:\n",
        "      import imageio\n",
        "      images = []\n",
        "      for e in range(train_epoch):\n",
        "          img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
        "          images.append(imageio.imread(img_name))\n",
        "      imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)\n",
        "    except Exception as e:\n",
        "      print(\"Haven't gotten imageio to work on the cluster, smh. Error message:\")\n",
        "      print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qS8yIT_8857t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3431
        },
        "outputId": "b8666ef9-0bf3-4ec1-982a-d66f208e8745"
      },
      "cell_type": "code",
      "source": [
        "if use_progressive_variant:\n",
        "  build_progr_gen_layers(combined_res_change_and_conv)\n",
        "  build_progr_dis_layers(combined_res_change_and_conv)\n",
        "  current_res = start_res\n",
        "  for counter in range(num_res_change):\n",
        "\n",
        "    imgs = load_dataset(current_res)\n",
        "    if counter == 0:\n",
        "      train(imgs, counter, False, image_size=current_res)\n",
        "    else:\n",
        "      train(imgs, counter, True, image_size=current_res)\n",
        "      train(imgs, counter, False, image_size=current_res)\n",
        "      \n",
        "    current_res = 2*current_res\n",
        "    if current_res == 128:\n",
        "      current_res = 125  # little res change s.t. final image size is 1000, not 1024\n",
        "      \n",
        "  \n",
        "else:\n",
        "  image_size=64\n",
        "  imgs = load_dataset(image_size=image_size)\n",
        "  train(imgs, image_size=image_size)\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'conv2d_29', 'trainable': True, 'dtype': None, 'filters': 2, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_30', 'trainable': True, 'dtype': None, 'filters': 2, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_31', 'trainable': True, 'dtype': None, 'filters': 4, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_32', 'trainable': True, 'dtype': None, 'filters': 4, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_33', 'trainable': True, 'dtype': None, 'filters': 4, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_34', 'trainable': True, 'dtype': None, 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_35', 'trainable': True, 'dtype': None, 'filters': 8, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_36', 'trainable': True, 'dtype': None, 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_37', 'trainable': True, 'dtype': None, 'filters': 16, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_38', 'trainable': True, 'dtype': None, 'filters': 16, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_39', 'trainable': True, 'dtype': None, 'filters': 16, 'kernel_size': (4, 4), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_40', 'trainable': True, 'dtype': None, 'filters': 32, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_41', 'trainable': True, 'dtype': None, 'filters': 32, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_42', 'trainable': True, 'dtype': None, 'filters': 32, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_43', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_44', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_45', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_46', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_47', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_48', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_49', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_50', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_51', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_52', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_53', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_54', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_55', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_56', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (1, 1), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'conv2d_57', 'trainable': True, 'dtype': None, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dense_1', 'trainable': True, 'dtype': None, 'units': 64, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "{'name': 'dense_2', 'trainable': True, 'dtype': None, 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "Downsampling input images to: 15x15\n",
            "Dataset not found. Using dummy dataset\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "[None, 4, 4, 64]\n",
            "[None, 4, 4, 64]\n",
            "Using LSGAN. Might still be wrong.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "(<tf.Tensor 'gradients/AddN_15:0' shape=(1, 1, 1, 64) dtype=float32>, <tf.Variable 'discriminator_1/conv2d_56/kernel:0' shape=(1, 1, 1, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN_14:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_1/conv2d_56/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN_7:0' shape=(3, 3, 65, 64) dtype=float32>, <tf.Variable 'discriminator_1/conv2d_57/kernel:0' shape=(3, 3, 65, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN_6:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_1/conv2d_57/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN_5:0' shape=(1024, 64) dtype=float32>, <tf.Variable 'discriminator_1/dense_1/kernel:0' shape=(1024, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN_4:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_1/dense_1/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN_1:0' shape=(64, 1) dtype=float32>, <tf.Variable 'discriminator_1/dense_2/kernel:0' shape=(64, 1) dtype=float32>)\n",
            "(<tf.Tensor 'gradients/AddN:0' shape=(1,) dtype=float32>, <tf.Variable 'discriminator_1/dense_2/bias:0' shape=(1,) dtype=float32>)\n",
            "training start!\n",
            "[1/5] - ptime: 1.80 loss_d: 0.744, loss_g: 0.511\n",
            "[2/5] - ptime: 1.32 loss_d: 0.672, loss_g: 0.404\n",
            "[3/5] - ptime: 1.31 loss_d: 0.664, loss_g: 0.387\n",
            "[4/5] - ptime: 1.33 loss_d: 0.499, loss_g: 0.433\n",
            "[5/5] - ptime: 1.34 loss_d: 0.378, loss_g: 0.437\n",
            "Avg per epoch ptime: 1.42, total 5 epochs ptime: 12.21\n",
            "Training finish!... save training results\n",
            "Downsampling input images to: 31x31\n",
            "Dataset not found. Using dummy dataset\n",
            "[None, 4, 4, 64]\n",
            "[None, 4, 4, 64]\n",
            "Using LSGAN. Might still be wrong.\n",
            "(None, <tf.Variable 'discriminator_1/conv2d_56/kernel:0' shape=(1, 1, 1, 64) dtype=float32>)\n",
            "(None, <tf.Variable 'discriminator_1/conv2d_56/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_7:0' shape=(3, 3, 65, 64) dtype=float32>, <tf.Variable 'discriminator_1/conv2d_57/kernel:0' shape=(3, 3, 65, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_6:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_1/conv2d_57/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_5:0' shape=(1024, 64) dtype=float32>, <tf.Variable 'discriminator_1/dense_1/kernel:0' shape=(1024, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_4:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_1/dense_1/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_1:0' shape=(64, 1) dtype=float32>, <tf.Variable 'discriminator_1/dense_2/kernel:0' shape=(64, 1) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN:0' shape=(1,) dtype=float32>, <tf.Variable 'discriminator_1/dense_2/bias:0' shape=(1,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_25:0' shape=(1, 1, 1, 64) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_53/kernel:0' shape=(1, 1, 1, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_24:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_53/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_21:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_54/kernel:0' shape=(3, 3, 64, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_20:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_54/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_17:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_55/kernel:0' shape=(3, 3, 64, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_16:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_55/bias:0' shape=(64,) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_15:0' shape=(1, 1, 1, 64) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_50/kernel:0' shape=(1, 1, 1, 64) dtype=float32>)\n",
            "(<tf.Tensor 'gradients_2/AddN_14:0' shape=(64,) dtype=float32>, <tf.Variable 'discriminator_3/conv2d_50/bias:0' shape=(64,) dtype=float32>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    512\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1052\u001b[0m           elems_as_tensors.append(\n\u001b[0;32m-> 1053\u001b[0;31m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0m\u001b[1;32m   1054\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 245\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: None values not supported.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    524\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 525\u001b[0;31m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1052\u001b[0m           elems_as_tensors.append(\n\u001b[0;32m-> 1053\u001b[0;31m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0m\u001b[1;32m   1054\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 245\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: None values not supported.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-11e22203ecb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-44e8f277b647>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(imgs, num_res_change_layers, cross_fade, device, image_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# create gradient summaries for tensorboard - HANNES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}_grad\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}grad\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/summary.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, values, collections, family)\u001b[0m\n\u001b[1;32m    175\u001b[0m       default_name='HistogramSummary') as (tag, scope):\n\u001b[1;32m    176\u001b[0m     val = _gen_logging_ops.histogram_summary(\n\u001b[0;32m--> 177\u001b[0;31m         tag=tag, values=values, name=scope)\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0m_summary_op_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUMMARIES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_logging_ops.py\u001b[0m in \u001b[0;36mhistogram_summary\u001b[0;34m(tag, values, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 312\u001b[0;31m         \"HistogramSummary\", tag=tag, values=values, name=name)\n\u001b[0m\u001b[1;32m    313\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    527\u001b[0m               raise ValueError(\n\u001b[1;32m    528\u001b[0m                   \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                   (input_name, err))\n\u001b[0m\u001b[1;32m    530\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    531\u001b[0m                       (input_name, op_type_name, observed))\n",
            "\u001b[0;31mValueError\u001b[0m: Tried to convert 'values' to a tensor and failed. Error: None values not supported."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oOrQyfQyz8ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*# Load Dataset*"
      ]
    },
    {
      "metadata": {
        "id": "L-QHIAkMkA39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "suMOrosIj8Br",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
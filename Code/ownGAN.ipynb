{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ownGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tillns/CIL_project/blob/master/Code/ownGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_f2HN8Mf4ydd",
        "colab_type": "code",
        "outputId": "2f3358e8-4bbd-4f3a-b5f6-bd9197bc8ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "use_progressive_variant = False\n",
        "\n",
        "# TODO: 1) add tensorboard support to better analyze training \n",
        "# 2) a progress bar would be kinda nice as well\n",
        "# 3) tweak params, of course\n",
        "# 4) think about batch norm/instance norm\n",
        "# 5) maybe add some normal conv layers to gen/dis\n",
        "# 6) name files based on parameters and architecture, s.t. there are individual ones for different kinds of training\n",
        "# 7) Adding noise to real input images before feeding them into the dis might help https://github.com/soumith/ganhacks/issues/14\n",
        "# 8) save model every few minutes maybe\n",
        "# 9) progressive growing proposed by Sven: https://github.com/tkarras/progressive_growing_of_gans --- Code at https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py\n",
        "#    not yet implemented use_wscale? weight decay 0.999 I think\n",
        "#    they seem to generally use float 32, maybe adjust code to that\n",
        "import os, time, itertools, pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "# for now, only those images with label 1 are kept\n",
        "# consider including also the fake ones\n",
        "\n",
        "original_image_size = 1000\n",
        "image_size = 64\n",
        "patch_divide_factor = 4\n",
        "factor_to_downsample = max(1024//image_size//patch_divide_factor, 1)\n",
        "downsample_size = original_image_size//factor_to_downsample  \n",
        "# e.g. for training on 64x64, the original images may be downsampled to 250x250\n",
        "# and during training. random 64x64 (appr. 1/4 = 1/patch_divide_factor) patches\n",
        "# are drawn from these images\n",
        "print(\"Downsampling input images to: {}x{}\".format(downsample_size, downsample_size))\n",
        "image_channels = 1\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "image_directory = os.path.join(home_dir, \"dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled\")\n",
        "label_path = os.path.join(home_dir, \"dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled.csv\")\n",
        "print(label_path)\n",
        "\n",
        "convs = []\n",
        "lrelus = []\n",
        "resblocks = []\n",
        "upsamples = []\n",
        "tf.reset_default_graph()\n",
        "tf.InteractiveSession().close()\n",
        "\n",
        "# params to tweak\n",
        "gen_num_upsample = 2\n",
        "gen_res = pow(2, gen_num_upsample)\n",
        "gen_num_resblocks = 0\n",
        "gen_num_normal_convs = 0\n",
        "dis_num_downsample = 2\n",
        "dis_res = pow(2, dis_num_downsample)\n",
        "dis_num_normal_convs = 0\n",
        "dis_num_resblocks = 0\n",
        "# my guess is that b&w star pics won't need many feature maps, bc the only \n",
        "# features are white dots and black background\n",
        "gen_start_features = 32\n",
        "dis_start_features = 16\n",
        "normal_conv_kernel_dim = 3\n",
        "normal_kernel = (normal_conv_kernel_dim, normal_conv_kernel_dim)\n",
        "downsample_conv_kernel_dim = 4\n",
        "lrelu_factor = 0.2\n",
        "batch_size = 32\n",
        "lr = 0.0002\n",
        "train_epoch = 200\n",
        "GAN_loss_type = 'LSGAN'\n",
        "\n",
        "# some params for progressive growing GAN\n",
        "max_features = 64\n",
        "min_features = 2\n",
        "start_res = 4\n",
        "# think about difference to 1000 (possibility is a conv that reduces 128 to 125,\n",
        "# which would require valid paddding with either a kernel size of 4x4 or \n",
        "# removing one line to achieve a 127x127 spatial res. Not sure which would be worse ^^)\n",
        "end_res = original_image_size  \n",
        "# for full res, the next two should be 9 and 4\n",
        "num_res_change = math.log(round(end_res/start_res, 0), 2)  \n",
        "num_const_feature_layers = min(4, num_res_change) \n",
        "mbstd_group_size = 4\n",
        "\n",
        "try:\n",
        "  f=open(label_path,'r')\n",
        "  label_list = []\n",
        "  for line in f:\n",
        "    if not \"Id,Actual\" in line:\n",
        "      label_list.append(line.split(\",\"))\n",
        "  label_list = sorted(label_list)\n",
        "  labels = np.zeros(len(label_list))\n",
        "  for ind, label in enumerate(label_list):\n",
        "    labels[ind] = label[1]\n",
        "\n",
        "  imgs = np.empty((0, downsample_size, downsample_size, image_channels)) \n",
        "  img_list = []\n",
        "  for filename in os.listdir(image_directory):\n",
        "    if filename.endswith(\".png\") and not filename.startswith(\"._\"):\n",
        "       img_list.append(filename)\n",
        "\n",
        "\n",
        "  img_list = sorted(img_list)\n",
        "  for ind, filename in enumerate(img_list):\n",
        "    if labels[ind] == 1:\n",
        "      img = Image.open(os.path.join(image_directory, filename)).resize((downsample_size, downsample_size))\n",
        "      imgs = np.append(imgs, np.array(img).reshape((1, downsample_size, downsample_size, image_channels)), axis=0)\n",
        "  # imgs = (imgs - 0.5) / 0.5  # if they go from 0 to 1, use this to make them go from -1 to 1\n",
        "\n",
        "  \n",
        "except FileNotFoundError:\n",
        "  print(\"Dataset not found. Using dummy dataset\")\n",
        "  labels = np.ones(1000)\n",
        "  imgs = np.random.normal(0, 1, (1000, downsample_size, downsample_size, image_channels))\n",
        "  \n",
        "\n",
        "# this could get useful if we include the fake images from the dataset with label 0 \n",
        "dataset_length = int(imgs.shape[0])\n",
        "labels_wide = np.zeros((dataset_length, image_size//dis_res, image_size//dis_res, 1))\n",
        "for ind in range(dataset_length):\n",
        "  labels_wide[ind, :, :, :] = labels[ind]\n",
        "\n",
        "#dataset = tf.data.Dataset.from_tensor_slices(imgs)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downsampling input images to: 250x250\n",
            "/root/dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled.csv\n",
            "Dataset not found. Using dummy dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OgUOcUh43yuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from random import randint\n",
        "\n",
        "def lrelu(x, th=0.2):\n",
        "    return tf.maximum(th * x, x)\n",
        "  \n",
        "def pixel_norm(x, epsilon=1e-8):\n",
        "  return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + epsilon)\n",
        "\n",
        "# Minibatch standard deviation.\n",
        "\n",
        "def minibatch_stddev_layer(x, group_size=4):\n",
        "    with tf.variable_scope('MinibatchStddev'):\n",
        "        group_size = tf.minimum(group_size, tf.shape(x)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "        s = x.shape                                             # [NCHW]  Input shape.\n",
        "        y = tf.reshape(x, [group_size, -1, s[1], s[2], s[3]])   # [GMCHW] Split minibatch into M groups of size G.\n",
        "        y = tf.cast(y, tf.float32)                              # [GMCHW] Cast to FP32.\n",
        "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMCHW] Subtract mean over group.\n",
        "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MCHW]  Calc variance over group.\n",
        "        y = tf.sqrt(y + 1e-8)                                   # [MCHW]  Calc stddev over group.\n",
        "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
        "        y = tf.cast(y, x.dtype)                                 # [M111]  Cast back to original data type.\n",
        "        y = tf.tile(y, [group_size, 1, s[2], s[3]])             # [N1HW]  Replicate over group and pixels.\n",
        "        return tf.concat([x, y], axis=1)                        # [NCHW]  Append as new fmap.\n",
        "\n",
        "      \n",
        "# in the progression paper, they apply the activation (lrelu) BEFORE the pixel normalization\n",
        "def add_activated_norm(x, lrelu_factor=0.2, isTrain=True, normalization='pixel'):\n",
        "  if normalization == 'pixel':\n",
        "    lrelus.append(pixelnorm(lrelu(x, lrelu_factor)))\n",
        "  elif normalization == 'batch':\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(x, training=isTrain), lrelu_factor))\n",
        "  else:\n",
        "    lrelus.append(x, lrelu_factor)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "# number of input features MUST equal number of output features!\n",
        "def get_resblock(x, features=256, kernel=[3, 3], strides=(1, 1), padding='same', lrelu_factor=0.2, isTrain=True):\n",
        "    convs.append(tf.layers.conv2d(x, features, kernel, strides, padding))\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "    convs.append(tf.layers.conv2d(lrelus[-1], features, kernel, strides, padding))\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "    return lrelus[-1]+x;\n",
        "  \n",
        "  \n",
        "# gen and dis after growing GANs paper. In the paper, they describe their up-/downsampling\n",
        "# methods to be separate from the conv while in the code, they also explore the combination\n",
        "# with transposed/strided convolution. Maybe add this here, it's more efficient.  \n",
        "\n",
        "# TODO: The rough architecture should be right now. Run and fix all errors.\n",
        "# Also, there is no implementation for the real progression part yet. For this, \n",
        "# some layers would probably have to load the saved weights from the training with \n",
        "# fewer layers. Not exactly sure how to do it yet.\n",
        "def generator_progr(x, isTrain=True, reuse=False, transp=False):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "      \n",
        "      features = gen_start_features\n",
        "      # first conv block\n",
        "      x = tf.reshape(x, (-1, features))  # this assumes the input to have 1x1 spatial res\n",
        "      x = tf.layers.dense(pixel_norm(x), features*start_res*start_res)\n",
        "      x = tf.reshape(x, (-1, features, start_res, start_res))\n",
        "      add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='pixel')\n",
        "      convs.append(tf.layers.conv2d(lrelus[-1], features, normal_kernel, padding='same'))\n",
        "      add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='pixel')\n",
        "      x = lrelus[-1]\n",
        "      for num_upsample in range(num_res_change):\n",
        "        if num_upsample >= num_const_feature_layers-1: \n",
        "          features = features//2\n",
        "          \n",
        "        if transp:\n",
        "          # Put transposed conv here. Should be more efficient but could also produce checkerboards, we'll see\n",
        "          pass\n",
        "        else:\n",
        "          upsamples.append(tf.image.resize(x, (2*x.shape[1], 2*x.shape[2])), method=1)\n",
        "          convs.append(tf.layers.conv2d(upsamples[-1], features, normal_kernel, padding='same'))\n",
        "        add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='pixel')\n",
        "        if start_res*pow(2, num_upsample) == 128:\n",
        "          # this should decrease the spatial resolution from 128 to 125 to produce 1000x1000 images in the end\n",
        "          convs.append(tf.layers.conv2d(lrelus[-1], features, kernel=(4, 4), padding='valid'))\n",
        "        else:\n",
        "          convs.append(tf.layers.conv2d(lrelus[-1], features, normal_kernel, padding='same'))\n",
        "        add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='pixel')\n",
        "        x = lrelus[-1]\n",
        "      \n",
        "      # this layer has to be cross faded for res changes\n",
        "      o = tf.layers.conv2d(x, image_channels, kernel=(1, 1), strides=(1, 1), padding='same')\n",
        "      # they mention a linear activation, which probably means none at all...\n",
        "      if False:  \n",
        "        o = tf.nn.tanh(conv5)\n",
        "\n",
        "      return o\n",
        "    \n",
        "# D(x)\n",
        "def discriminator_progr(x, isTrain=True, reuse=False, strided_conv=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "      features = dis_start_features\n",
        "      # this layer has to be cross faided for res changes\n",
        "      x = tf.layers.conv2d(x, features, kernel=(1, 1), padding='same')\n",
        "      add_activated_norm(x, lrelu_factor, isTrain, normalization='None')\n",
        "      \n",
        "      # downsampling convs (reduce resolution, increse features)\n",
        "      for num_downsample in range(num_res_change):\n",
        "        if end_res//pow(2, num_downsample) == 125:\n",
        "          # padding: 125x125 -> 131x131, then valid padding conv 4x4: 131x131 -> 128x128. CHECK\n",
        "          x = tf.pad(x, tf.constant([[0, 0], [0, 0], [3, 3], [3, 3]])) \n",
        "          convs.append(tf.layers.conv2d(x, features, kernel=(4, 4), padding='valid'))\n",
        "        else:\n",
        "          convs.append(tf.layers.conv2d(x, features, normal_kernel, padding='same'))\n",
        "        add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='None')\n",
        "        if num_downsample <= num_res_change-num_const_feature_layers:\n",
        "          features=features*2\n",
        "        if strided_conv:\n",
        "          # add strided conv\n",
        "          pass\n",
        "        else:\n",
        "          convs.append(tf.layers.conv2d(lrelus[-1], features, normal_kernel, padding='same'))\n",
        "          add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='None')\n",
        "          x = tf.layers.average_pooling2d(lrelus[-1], 2, 2)\n",
        "\n",
        "      if mbstd_group_size > 1:\n",
        "        x = minibatch_stddev_layer(x, mbstd_group_size)\n",
        "      convs.append(tf.layers.conv2d(x, features, normal_kernel, strides=(1, 1), padding='same'))\n",
        "      x = tf.reshape(convs[-1], (-1, features*start_res*start_res))  # assumes input to be featuresx4x4\n",
        "      x = tf.layers.dense(x, features)\n",
        "      add_activated_norm(x, lrelu_factor, isTrain, normalization='None')\n",
        "      o = tf.layers.dense(x, 1)\n",
        "\n",
        "      return o\n",
        "  \n",
        "      \n",
        "    \n",
        "# G(z)\n",
        "def generator(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "      \n",
        "      features = gen_start_features\n",
        "      # resblocks (num features kept constant)\n",
        "      for num_resblock in range(gen_num_resblocks):\n",
        "          if num_resblock == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = resblocks[-1]\n",
        "          resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # upsample + conv (num features reduced)\n",
        "      for num_upsample in range(gen_num_upsample):\n",
        "        if num_upsample == 0:\n",
        "          if gen_num_resblocks == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = resblocks[-1]\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        upsamples.append(tf.image.resize_images(input_im, (2*int(input_im.shape[1]), 2*int(input_im.shape[2])), method=1))\n",
        "        convs.append(tf.layers.conv2d(upsamples[-1], features, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same'))\n",
        "        # maybe change to instance normalization (tf.contrib.layers.instance_norm(input, trainable)) \n",
        "        # this does not exist in tf 1.3 yet and I haven't gotten it to work on 1.13 either\n",
        "        # error probably lies in trainable, haven't understood that yet\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features//2\n",
        "\n",
        "      # output layer\n",
        "      if gen_num_upsample == 0:\n",
        "        if gen_num_resblocks == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "      else:\n",
        "        input_im = lrelus[-1]\n",
        "      conv5 = tf.layers.conv2d(lrelus[-1], 1, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      # never normalize output\n",
        "      o = tf.nn.tanh(conv5)\n",
        "\n",
        "      return o\n",
        "\n",
        "# D(x)\n",
        "def discriminator(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "      \n",
        "      features = dis_start_features\n",
        "      # downsampling convs (reduce resolution, increse features)\n",
        "      for num_downsample in range(dis_num_downsample):\n",
        "        if num_downsample == 0:\n",
        "            input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        convs.append(tf.layers.conv2d(input_im, features, [downsample_conv_kernel_dim, downsample_conv_kernel_dim], strides=(2, 2), padding='same'))\n",
        "        # maybe change to instance normalization\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features*2\n",
        "        \n",
        "      # resblocks (features kept constant)  \n",
        "      for num_resblock in range(dis_num_resblocks):\n",
        "        if num_resblock == 0:\n",
        "          if dis_num_downsample == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = lrelus[-1]\n",
        "            features=features//2\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "        resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # output layer without fc, s.t. local patches are rated and generator is\n",
        "      # forced to be cinsistent everywhere (PatchGAN). Could be changed tho...\n",
        "      if dis_num_resblocks == 0:\n",
        "        if dis_num_downsample == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "      else:\n",
        "        input_im = resblocks[-1]\n",
        "      conv6 = tf.layers.conv2d(input_im, 1, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      #fc = tf.layers.dense(conv6, 32*32, activation='sigmoid')\n",
        "      o = tf.nn.sigmoid(conv6)\n",
        "\n",
        "      return o\n",
        "    \n",
        "def D(x, isTrain=True, reuse=False):\n",
        "  if use_progressive_variant:\n",
        "    return discriminator_progr(x, isTrain, reuse)\n",
        "  else:\n",
        "    return discriminator(x, isTrain, reuse)\n",
        "  \n",
        "def G(x, isTrain=True, reuse=False):\n",
        "  if use_progressive_variant:\n",
        "    return generator_progr(x, isTrain, reuse)\n",
        "  else:\n",
        "    return generator(x, isTrain, reuse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tgIt_tmMc5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the WGAN-GP loss for the generator and discriminator, copied from the growing GAN paper. "
      ]
    },
    {
      "metadata": {
        "id": "sp4PP9qqMplf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lerp(a, b, t):\n",
        "    with tf.name_scope('Lerp'):\n",
        "        return a + (b - a) * t\n",
        "      \n",
        "def autosummary(name, value):\n",
        "    id = name.replace('/', '_')\n",
        "    if is_tf_expression(value):\n",
        "        with tf.name_scope('summary_' + id), tf.device(value.device):\n",
        "            update_op = _create_autosummary_var(name, value)\n",
        "            with tf.control_dependencies([update_op]):\n",
        "                return tf.identity(value)\n",
        "    else: # python scalar or numpy array\n",
        "        if name not in _autosummary_immediate:\n",
        "            with absolute_name_scope('Autosummary/' + id), tf.device(None), tf.control_dependencies(None):\n",
        "                update_value = tf.placeholder(tf.float32)\n",
        "                update_op = _create_autosummary_var(name, update_value)\n",
        "                _autosummary_immediate[name] = update_op, update_value\n",
        "        update_op, update_value = _autosummary_immediate[name]\n",
        "        run(update_op, {update_value: np.float32(value)})\n",
        "        return value\n",
        "      \n",
        "def fp32(*values):\n",
        "    if len(values) == 1 and isinstance(values[0], tuple):\n",
        "        values = values[0]\n",
        "    values = tuple(tf.cast(v, tf.float32) for v in values)\n",
        "    return values if len(values) >= 2 else values[0]\n",
        "\n",
        "# wgangp losses not right yet\n",
        "def G_wgangp(\n",
        "    cond_weight = 1.0): # Weight of the conditioning term.\n",
        "\n",
        "    latents = tf.random_normal([batch_size][image_size])  \n",
        "    labels = training_set.get_random_labels_tf(minibatch_size)\n",
        "    fake_images_out = G(latents, labels, is_training=True)\n",
        "    fake_scores_out, fake_labels_out = fp32(D(fake_images_out, is_training=True))\n",
        "    loss = -fake_scores_out\n",
        "\n",
        "    #if D.output_shapes[1][1] > 0: <. don't know...\n",
        "    with tf.name_scope('LabelPenalty'):\n",
        "        label_penalty_fakes = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fake_labels_out)\n",
        "    loss += label_penalty_fakes * cond_weight\n",
        "    return loss\n",
        "\n",
        "\n",
        "def D_wgangp(G, D, opt, training_set, minibatch_size, reals, labels,\n",
        "    wgan_lambda     = 10.0,     # Weight for the gradient penalty term.\n",
        "    wgan_epsilon    = 0.001,    # Weight for the epsilon term, \\epsilon_{drift}.\n",
        "    wgan_target     = 1.0,      # Target value for gradient magnitudes.\n",
        "    cond_weight     = 1.0):     # Weight of the conditioning terms.\n",
        "\n",
        "    latents = tf.random_normal([minibatch_size] + G.input_shapes[0][1:])\n",
        "    fake_images_out = G.get_output_for(latents, labels, is_training=True)\n",
        "    real_scores_out, real_labels_out = fp32(D.get_output_for(reals, is_training=True))\n",
        "    fake_scores_out, fake_labels_out = fp32(D.get_output_for(fake_images_out, is_training=True))\n",
        "    real_scores_out = tfutil.autosummary('Loss/real_scores', real_scores_out)\n",
        "    fake_scores_out = tfutil.autosummary('Loss/fake_scores', fake_scores_out)\n",
        "    loss = fake_scores_out - real_scores_out\n",
        "\n",
        "    with tf.name_scope('GradientPenalty'):\n",
        "        mixing_factors = tf.random_uniform([minibatch_size, 1, 1, 1], 0.0, 1.0, dtype=fake_images_out.dtype)\n",
        "        mixed_images_out = tfutil.lerp(tf.cast(reals, fake_images_out.dtype), fake_images_out, mixing_factors)\n",
        "        mixed_scores_out, mixed_labels_out = fp32(D.get_output_for(mixed_images_out, is_training=True))\n",
        "        mixed_scores_out = tfutil.autosummary('Loss/mixed_scores', mixed_scores_out)\n",
        "        mixed_loss = opt.apply_loss_scaling(tf.reduce_sum(mixed_scores_out))\n",
        "        mixed_grads = opt.undo_loss_scaling(fp32(tf.gradients(mixed_loss, [mixed_images_out])[0]))\n",
        "        mixed_norms = tf.sqrt(tf.reduce_sum(tf.square(mixed_grads), axis=[1,2,3]))\n",
        "        mixed_norms = tfutil.autosummary('Loss/mixed_norms', mixed_norms)\n",
        "        gradient_penalty = tf.square(mixed_norms - wgan_target)\n",
        "    loss += gradient_penalty * (wgan_lambda / (wgan_target**2))\n",
        "\n",
        "    with tf.name_scope('EpsilonPenalty'):\n",
        "        epsilon_penalty = tfutil.autosummary('Loss/epsilon_penalty', tf.square(real_scores_out))\n",
        "    loss += epsilon_penalty * wgan_epsilon\n",
        "\n",
        "    if D.output_shapes[1][1] > 0:\n",
        "        with tf.name_scope('LabelPenalty'):\n",
        "            label_penalty_reals = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=real_labels_out)\n",
        "            label_penalty_fakes = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fake_labels_out)\n",
        "            label_penalty_reals = tfutil.autosummary('Loss/label_penalty_reals', label_penalty_reals)\n",
        "            label_penalty_fakes = tfutil.autosummary('Loss/label_penalty_fakes', label_penalty_fakes)\n",
        "        loss += (label_penalty_reals + label_penalty_fakes) * cond_weight\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_W3y6vvSQGN",
        "colab_type": "code",
        "outputId": "4d3a508f-3048-4a97-8b04-958a7e4b26ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1153
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "size_figure_grid = 5\n",
        "fixed_z_ = np.random.normal(0, 1, (size_figure_grid*size_figure_grid, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
        "    test_images = sess.run(G_z, {z: fixed_z_, isTrain: False})\n",
        "\n",
        "    fig_size = image_size*size_figure_grid*1.3/100  # appr. formula s.t. image sizes aren't reduced\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(fig_size, fig_size))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(size_figure_grid*size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (image_size, image_size)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "# plots graph with dis and gen loss \n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "#with tf.device('/cpu:0'):\n",
        "with tf.device('/device:GPU:0'): \n",
        "# variables : input\n",
        "  x = tf.placeholder(tf.float32, shape=(None, image_size, image_size, image_channels))\n",
        "  z = tf.placeholder(tf.float32, shape=(None, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "  isTrain = tf.placeholder(dtype=tf.bool)\n",
        "\n",
        "  # networks : generator\n",
        "  G_z = generator(z, isTrain)\n",
        "\n",
        "  # networks : discriminator\n",
        "  D_real_logits = discriminator(x, isTrain)\n",
        "  D_fake_logits = discriminator(G_z, isTrain, reuse=True)\n",
        "\n",
        "  # loss for each network\n",
        "  # think about GAN objective\n",
        "  if GAN_loss_type == 'LSGAN':\n",
        "    D_loss = tf.reduce_mean(tf.square(D_fake_logits)) + tf.reduce_mean(tf.square(D_real_logits - 1))\n",
        "    G_loss = tf.reduce_mean(tf.square(D_fake_logits - 1))\n",
        "  elif GAN_loss_type == 'WGAN-GP':\n",
        "    \n",
        "  else:\n",
        "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, image_size//dis_res, image_size//dis_res, 1])))\n",
        "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, image_size//dis_res, image_size//dis_res, 1])))\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, image_size//dis_res, image_size//dis_res, 1])))\n",
        "\n",
        "  # trainable variables for each network\n",
        "  T_vars = tf.trainable_variables()\n",
        "  D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
        "  G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
        "\n",
        "  # optimizer for each network\n",
        "  # This should probably not be experimented with. ADAM is standard nowadays.\n",
        "  with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "      D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n",
        "      G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(G_loss, var_list=G_vars)\n",
        "\n",
        "  # open session and initialize all variables\n",
        "  sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
        "  tf.global_variables_initializer().run()\n",
        "\n",
        "  # resize and normalization\n",
        "  # train_set = dataset#.eval()\n",
        "  #train_set = (train_set - 0.5) / 0.5  # normalization; range: -1 ~ 1\n",
        "\n",
        "  # results save folder\n",
        "  root = 'DCGAN_results/'\n",
        "  model = 'DCGAN_'\n",
        "  if not os.path.isdir(root):\n",
        "      os.mkdir(root)\n",
        "  if not os.path.isdir(root + 'Fixed_results'):\n",
        "      os.mkdir(root + 'Fixed_results')\n",
        "\n",
        "  train_hist = {}\n",
        "  train_hist['D_losses'] = []\n",
        "  train_hist['G_losses'] = []\n",
        "  train_hist['per_epoch_ptimes'] = []\n",
        "  train_hist['total_ptime'] = []\n",
        "\n",
        "  # training-loop\n",
        "  np.random.seed(int(time.time()))\n",
        "  print('training start!')\n",
        "  start_time = time.time()\n",
        "  for epoch in range(train_epoch):\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "    epoch_start_time = time.time()\n",
        "    #dataset.shuffle(buffer_size=dataset_length)\n",
        "    #dataset = dataset.batch(batch_size)\n",
        "    #iterator = dataset.make_one_shot_iterator()\n",
        "\n",
        "    # TODO: include progress bar\n",
        "    # to avoid overfitting, the images are shuffled every epoch\n",
        "    # if fake images are included, suffle an index list to access the corresponding label along with the image\n",
        "    np.random.shuffle(imgs)\n",
        "    for iteration in range(dataset_length // batch_size):\n",
        "      #x_ = iterator.get_next()\n",
        "      #x_ = x_.eval()\n",
        "\n",
        "      # update discriminator\n",
        "      x_ = imgs[iteration*batch_size:(iteration+1)*batch_size]\n",
        "      random_startx = randint(0, downsample_size-image_size)\n",
        "      random_starty = randint(0, downsample_size-image_size)\n",
        "      # draw a random patch from the input\n",
        "      x_ = x_[:, random_startx:random_startx+image_size, random_starty:random_starty+image_size]\n",
        "      for i in range(2):\n",
        "        if randint(0, 1):\n",
        "          x_ = np.flip(x_, axis=i+1)  # randomly flip x- and y-axis\n",
        "      z_ = np.random.normal(0, 1, (batch_size, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "\n",
        "      loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, isTrain: True})\n",
        "      D_losses.append(loss_d_)\n",
        "\n",
        "      # update generator\n",
        "      z_ = np.random.normal(0, 1, (batch_size, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "      loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, isTrain: True})\n",
        "      G_losses.append(loss_g_)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n",
        "    fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n",
        "    show_result((epoch + 1), save=True, show=False, path=fixed_p)\n",
        "    train_hist['D_losses'].append(np.mean(D_losses))\n",
        "    train_hist['G_losses'].append(np.mean(G_losses))\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "  end_time = time.time()\n",
        "  total_ptime = end_time - start_time\n",
        "  train_hist['total_ptime'].append(total_ptime)\n",
        "\n",
        "  print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n",
        "  print(\"Training finish!... save training results\")\n",
        "  with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
        "      pickle.dump(train_hist, f)\n",
        "\n",
        "  show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
        "\n",
        "  # this thing makes a gif out of the results after every epoch, so it's not\n",
        "  # necessary \n",
        "  try:\n",
        "    import imageio\n",
        "    images = []\n",
        "    for e in range(train_epoch):\n",
        "        img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
        "        images.append(imageio.imread(img_name))\n",
        "    imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)\n",
        "  except Exception as e:\n",
        "    print(\"Haven't gotten imageio to work on the cluster, smh. Error message:\")\n",
        "    print(e)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-a34b848bdc5c>:160: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-3-a34b848bdc5c>:164: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "training start!\n",
            "[1/200] - ptime: 32.36 loss_d: 0.549, loss_g: 0.232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a34b848bdc5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m       \u001b[0;31m# update generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       \u001b[0mz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgen_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgen_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_start_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m       \u001b[0mloss_g_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_optim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0mG_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_g_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oOrQyfQyz8ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*# Load Dataset*"
      ]
    },
    {
      "metadata": {
        "id": "L-QHIAkMkA39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "suMOrosIj8Br",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
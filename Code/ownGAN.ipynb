{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ownGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tillns/CIL_project/blob/master/Code/ownGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_f2HN8Mf4ydd",
        "colab_type": "code",
        "outputId": "7fa71ecd-9f33-401e-d08b-ef9d99d3d905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# TODO: 1) add tensorboard support to better analyze training \n",
        "# 2) a progress bar would be kinda nice as well\n",
        "# 3) tweak params, of course\n",
        "# 4) think about batch norm/instance norm\n",
        "# 5) maybe add some normal conv layers to gen/dis\n",
        "# 6) name files based on parameters and architecture, s.t. there are individual ones for different kinds of training\n",
        "# 7) Adding noise to real input images before feeding them into the dis might help https://github.com/soumith/ganhacks/issues/14\n",
        "# 8) save model every few minutes maybe\n",
        "# 9) progressive growing proposed by Sven: https://github.com/tkarras/progressive_growing_of_gans --- Code at https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py\n",
        "#    not yet implemented use_wscale? weight decay 0.999 I think\n",
        "import os, time, itertools, pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# for now, only those images with label 1 are kept\n",
        "# consider including also the fake ones\n",
        "\n",
        "original_image_size = 1000\n",
        "image_size = 64\n",
        "patch_divide_factor = 4\n",
        "factor_to_downsample = 1024//image_size//patch_divide_factor\n",
        "downsample_size = original_image_size//factor_to_downsample  \n",
        "# e.g. for training on 64x64, the original images may be downsampled to 250x250\n",
        "# and during training. random 64x64 (appr. 1/4 = 1/patch_divide_factor) patches\n",
        "# are drawn from these images\n",
        "print(\"Downsampling input images to: {}x{}\".format(downsample_size, downsample_size))\n",
        "image_channels = 1\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "image_directory = os.path.join(home_dir, \"dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled\")\n",
        "label_path = os.path.join(home_dir, \"dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled.csv\")\n",
        "print(label_path)\n",
        "\n",
        "convs = []\n",
        "lrelus = []\n",
        "resblocks = []\n",
        "upsamples = []\n",
        "tf.reset_default_graph()\n",
        "tf.InteractiveSession().close()\n",
        "\n",
        "# params to tweak\n",
        "gen_num_upsample = 2\n",
        "gen_res = pow(2, gen_num_upsample)\n",
        "gen_num_resblocks = 0\n",
        "gen_num_normal_convs = 0\n",
        "dis_num_downsample = 2\n",
        "dis_res = pow(2, dis_num_downsample)\n",
        "dis_num_normal_convs = 0\n",
        "dis_num_resblocks = 0\n",
        "# my guess is that b&w star pics won't need many feature maps, bc the only \n",
        "# features are white dots and black background\n",
        "gen_start_features = 32\n",
        "dis_start_features = 16\n",
        "normal_conv_kernel_dim = 3\n",
        "downsample_conv_kernel_dim = 4\n",
        "lrelu_factor = 0.2\n",
        "batch_size = 32\n",
        "lr = 0.0002\n",
        "train_epoch = 200\n",
        "\n",
        "try:\n",
        "  f=open(label_path,'r')\n",
        "  label_list = []\n",
        "  for line in f:\n",
        "    if not \"Id,Actual\" in line:\n",
        "      label_list.append(line.split(\",\"))\n",
        "  label_list = sorted(label_list)\n",
        "  labels = np.zeros(len(label_list))\n",
        "  for ind, label in enumerate(label_list):\n",
        "    labels[ind] = label[1]\n",
        "\n",
        "  imgs = np.empty((0, downsample_size, downsample_size, image_channels)) \n",
        "  img_list = []\n",
        "  for filename in os.listdir(image_directory):\n",
        "    if filename.endswith(\".png\") and not filename.startswith(\"._\"):\n",
        "       img_list.append(filename)\n",
        "\n",
        "\n",
        "  img_list = sorted(img_list)\n",
        "  for ind, filename in enumerate(img_list):\n",
        "    if labels[ind] == 1:\n",
        "      img = Image.open(os.path.join(image_directory, filename)).resize((downsample_size, downsample_size))\n",
        "      imgs = np.append(imgs, np.array(img).reshape((1, downsample_size, downsample_size, image_channels)), axis=0)\n",
        "  # imgs = (imgs - 0.5) / 0.5  # if they go from 0 to 1, use this to make them go from -1 to 1\n",
        "\n",
        "  \n",
        "except FileNotFoundError:\n",
        "  print(\"Dataset not found. Using dummy dataset\")\n",
        "  labels = np.ones(1000)\n",
        "  imgs = np.random.normal(0, 1, (1000, image_size, image_size, image_channels))\n",
        "  \n",
        "\n",
        "# this could get useful if we include the fake images from the dataset with label 0 \n",
        "dataset_length = int(imgs.shape[0])\n",
        "labels_wide = np.zeros((dataset_length, image_size//dis_res, image_size//dis_res, 1))\n",
        "for ind in range(dataset_length):\n",
        "  labels_wide[ind, :, :, :] = labels[ind]\n",
        "\n",
        "#dataset = tf.data.Dataset.from_tensor_slices(imgs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downsampling input images to: 250x250\n",
            "/home/tillns/dataset/cil-cosmology-2018/cosmology_aux_data_170429/labeled.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w_W3y6vvSQGN",
        "colab_type": "code",
        "outputId": "e6df5d95-f372-4a4d-cf80-4e5fbcf4c2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3598
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from random import randint\n",
        "\n",
        "def lrelu(x, th=0.2):\n",
        "    return tf.maximum(th * x, x)\n",
        "  \n",
        "def pixel_norm(x, epsilon=1e-8):\n",
        "  return x * tf.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + epsilon)\n",
        "\n",
        "# Minibatch standard deviation.\n",
        "\n",
        "def minibatch_stddev_layer(x, group_size=4):\n",
        "    with tf.variable_scope('MinibatchStddev'):\n",
        "        group_size = tf.minimum(group_size, tf.shape(x)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "        s = x.shape                                             # [NCHW]  Input shape.\n",
        "        y = tf.reshape(x, [group_size, -1, s[1], s[2], s[3]])   # [GMCHW] Split minibatch into M groups of size G.\n",
        "        y = tf.cast(y, tf.float32)                              # [GMCHW] Cast to FP32.\n",
        "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMCHW] Subtract mean over group.\n",
        "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MCHW]  Calc variance over group.\n",
        "        y = tf.sqrt(y + 1e-8)                                   # [MCHW]  Calc stddev over group.\n",
        "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
        "        y = tf.cast(y, x.dtype)                                 # [M111]  Cast back to original data type.\n",
        "        y = tf.tile(y, [group_size, 1, s[2], s[3]])             # [N1HW]  Replicate over group and pixels.\n",
        "        return tf.concat([x, y], axis=1)                        # [NCHW]  Append as new fmap.\n",
        "\n",
        "def add_activated_norm(x, lrelu_factor=0.2, isTrain=True, normalization='pixel'):\n",
        "  if normalization == 'pixel':\n",
        "    x = pixelnorm(x)\n",
        "  elif normalization == 'batch'\n",
        "    x = tf.layers.batch_normalization(convs[-1], training=isTrain)\n",
        "\n",
        "  lrelus.append(lrelu(x, lrelu_factor))\n",
        "  \n",
        "\n",
        "# number of input features MUST equal number of output features!\n",
        "def get_resblock(x, features=256, kernel=[3, 3], strides=(1, 1), padding='same', lrelu_factor=0.2, isTrain=True):\n",
        "    convs.append(tf.layers.conv2d(x, features, kernel, strides, padding))\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "    convs.append(tf.layers.conv2d(lrelus[-1], features, kernel, strides, padding))\n",
        "    lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "    return lrelus[-1]+x;\n",
        "  \n",
        "  \n",
        "# Conv blocks after growing GANs paper. In the paper, they describe their up-/downsampling\n",
        "# methods to be separate from the conv while in the code, they also explore the combination\n",
        "# with transposed/strided convolution. Maybe add this here, it's more efficient\n",
        "def get_convblock(x, features, upsample=True, kernel=[3, 3], strides=(1, 1), padding='same', lrelu_factor=0.2, nomalization='pixel', isTrain=True):\n",
        "  if upsample\n",
        "    upsamples.append(tf.image.resize(x, (2*x.shape[1], 2*x.shape[2])), method=1)\n",
        "  convs.append(tf.layers.conv2d(upsamples[-1], features, kernel, strides, padding))\n",
        "  add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization)\n",
        "  convs.append(tf.layers.conv2d(lrelus[-1], features, kernel, strides, padding))\n",
        "  add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization)\n",
        "  if not upsample:\n",
        "    return tf.layers.average_pooling2d(lrelus[-1], 2, 2)  # not completely sure about second 2\n",
        "  return lrelus[-1]\n",
        "  \n",
        "  \n",
        "def generator_progr(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "      \n",
        "      features = gen_start_features\n",
        "      # first conv block\n",
        "      x = tf.reshape(x, (-1, features))  # this assumes the input to have 1x1 spatial res\n",
        "      x = tf.layers.dense(pixel_norm(x), features*16)\n",
        "      x = tf.reshape(x, (-1, features, 4, 4))\n",
        "      convs.append(tf.layers.conv2d(x, features, kernel=[4, 4], stride=(1, 1), padding='same'))\n",
        "      add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization='pixel')\n",
        "      convs.append(tf.layers.conv2d(lrelus[-1], features, kernel, stride=(1, 1), padding='same'))\n",
        "      add_activated_norm(convs[-1], lrelu_factor, isTrain, normalization)\n",
        "      x = lrelus[-1]\n",
        "      for num_upsample in range(gen_num_upsample):\n",
        "        if num_upsample >= 3: \n",
        "          features = features//2\n",
        "        x = get_comvblock(x, features, upsample=True)\n",
        "      \n",
        "      # this layer has to be cross faided for res changes\n",
        "      o = tf.layers.conv2d(x, image_channels, kernel=(1, 1), stides=(1, 1), padding='same')\n",
        "      # they mention a linear activation, which probably means none at all...\n",
        "      if False:  \n",
        "        o = tf.nn.tanh(conv5)\n",
        "\n",
        "      return o\n",
        "    \n",
        "# D(x)\n",
        "def discriminator_progr(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "      \n",
        "      features = dis_start_features\n",
        "      # downsampling convs (reduce resolution, increse features)\n",
        "      for num_downsample in range(dis_num_downsample):\n",
        "        if num_downsample == 0:\n",
        "            input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        convs.append(tf.layers.conv2d(input_im, features, [downsample_conv_kernel_dim, downsample_conv_kernel_dim], strides=(2, 2), padding='same'))\n",
        "        # maybe change to instance normalization\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features*2\n",
        "        \n",
        "      # resblocks (features kept constant)  \n",
        "      for num_resblock in range(dis_num_resblocks):\n",
        "        if num_resblock == 0:\n",
        "          if dis_num_downsample == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = lrelus[-1]\n",
        "            features=features//2\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "        resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # output layer without fc, s.t. local patches are rated and generator is\n",
        "      # forced to be cinsistent everywhere (PatchGAN). Could be changed tho...\n",
        "      if dis_num_resblocks == 0:\n",
        "        if dis_num_downsample == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "      else:\n",
        "        input_im = resblocks[-1]\n",
        "      conv6 = tf.layers.conv2d(input_im, 1, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      #fc = tf.layers.dense(conv6, 32*32, activation='sigmoid')\n",
        "      o = tf.nn.sigmoid(conv6)\n",
        "\n",
        "      return o\n",
        "  \n",
        "      \n",
        "    \n",
        "# G(z)\n",
        "def generator(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "      \n",
        "      features = gen_start_features\n",
        "      # resblocks (num features kept constant)\n",
        "      for num_resblock in range(gen_num_resblocks):\n",
        "          if num_resblock == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = resblocks[-1]\n",
        "          resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # upsample + conv (num features reduced)\n",
        "      for num_upsample in range(gen_num_upsample):\n",
        "        if num_upsample == 0:\n",
        "          if gen_num_resblocks == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = resblocks[-1]\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        upsamples.append(tf.image.resize_images(input_im, (2*int(input_im.shape[1]), 2*int(input_im.shape[2])), method=1))\n",
        "        convs.append(tf.layers.conv2d(upsamples[-1], features, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same'))\n",
        "        # maybe change to instance normalization (tf.contrib.layers.instance_norm(input, trainable)) \n",
        "        # this does not exist in tf 1.3 yet and I haven't gotten it to work on 1.13 either\n",
        "        # error probably lies in trainable, haven't understood that yet\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features//2\n",
        "\n",
        "      # output layer\n",
        "      if gen_num_upsample == 0:\n",
        "        if gen_num_resblocks == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "      else:\n",
        "        input_im = lrelus[-1]\n",
        "      conv5 = tf.layers.conv2d(lrelus[-1], 1, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      # never normalize output\n",
        "      o = tf.nn.tanh(conv5)\n",
        "\n",
        "      return o\n",
        "\n",
        "# D(x)\n",
        "def discriminator(x, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "      \n",
        "      features = dis_start_features\n",
        "      # downsampling convs (reduce resolution, increse features)\n",
        "      for num_downsample in range(dis_num_downsample):\n",
        "        if num_downsample == 0:\n",
        "            input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "        convs.append(tf.layers.conv2d(input_im, features, [downsample_conv_kernel_dim, downsample_conv_kernel_dim], strides=(2, 2), padding='same'))\n",
        "        # maybe change to instance normalization\n",
        "        lrelus.append(lrelu(tf.layers.batch_normalization(convs[-1], training=isTrain), lrelu_factor))\n",
        "        features = features*2\n",
        "        \n",
        "      # resblocks (features kept constant)  \n",
        "      for num_resblock in range(dis_num_resblocks):\n",
        "        if num_resblock == 0:\n",
        "          if dis_num_downsample == 0:\n",
        "            input_im = x\n",
        "          else:\n",
        "            input_im = lrelus[-1]\n",
        "            features=features//2\n",
        "        else:\n",
        "          input_im = resblocks[-1]\n",
        "        resblocks.append(get_resblock(input_im, features=features, kernel = [normal_conv_kernel_dim, normal_conv_kernel_dim], lrelu_factor=lrelu_factor, isTrain=isTrain))\n",
        "\n",
        "      # output layer without fc, s.t. local patches are rated and generator is\n",
        "      # forced to be cinsistent everywhere (PatchGAN). Could be changed tho...\n",
        "      if dis_num_resblocks == 0:\n",
        "        if dis_num_downsample == 0:\n",
        "          input_im = x\n",
        "        else:\n",
        "          input_im = lrelus[-1]\n",
        "      else:\n",
        "        input_im = resblocks[-1]\n",
        "      conv6 = tf.layers.conv2d(input_im, 1, [normal_conv_kernel_dim, normal_conv_kernel_dim], strides=(1, 1), padding='same')\n",
        "      #fc = tf.layers.dense(conv6, 32*32, activation='sigmoid')\n",
        "      o = tf.nn.sigmoid(conv6)\n",
        "\n",
        "      return o\n",
        "\n",
        "size_figure_grid = 5\n",
        "fixed_z_ = np.random.normal(0, 1, (size_figure_grid*size_figure_grid, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
        "    test_images = sess.run(G_z, {z: fixed_z_, isTrain: False})\n",
        "\n",
        "    fig_size = image_size*size_figure_grid*1.3/100  # appr. formula s.t. image sizes aren't reduced\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(fig_size, fig_size))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(size_figure_grid*size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (image_size, image_size)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "# plots graph with dis and gen loss \n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "        \n",
        "#with tf.device('/cpu:0'):\n",
        "with tf.device('/device:GPU:0'): \n",
        "# variables : input\n",
        "  x = tf.placeholder(tf.float32, shape=(None, image_size, image_size, image_channels))\n",
        "  z = tf.placeholder(tf.float32, shape=(None, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "  isTrain = tf.placeholder(dtype=tf.bool)\n",
        "\n",
        "  # networks : generator\n",
        "  G_z = generator(z, isTrain)\n",
        "\n",
        "  # networks : discriminator\n",
        "  D_real_logits = discriminator(x, isTrain)\n",
        "  D_fake_logits = discriminator(G_z, isTrain, reuse=True)\n",
        "\n",
        "  # loss for each network\n",
        "  # think about these losses, there are many variants (vanilla GAN, WGAN, ...)\n",
        "  D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, image_size//dis_res, image_size//dis_res, 1])))\n",
        "  D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, image_size//dis_res, image_size//dis_res, 1])))\n",
        "  D_loss = D_loss_real + D_loss_fake\n",
        "  G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, image_size//dis_res, image_size//dis_res, 1])))\n",
        "\n",
        "  # trainable variables for each network\n",
        "  T_vars = tf.trainable_variables()\n",
        "  D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
        "  G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
        "\n",
        "  # optimizer for each network\n",
        "  # This should probably not be experimented with. ADAM is standard nowadays.\n",
        "  with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "      D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n",
        "      G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(G_loss, var_list=G_vars)\n",
        "\n",
        "  # open session and initialize all variables\n",
        "  sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=False, log_device_placement=True))\n",
        "  tf.global_variables_initializer().run()\n",
        "\n",
        "  # resize and normalization\n",
        "  # train_set = dataset#.eval()\n",
        "  #train_set = (train_set - 0.5) / 0.5  # normalization; range: -1 ~ 1\n",
        "\n",
        "  # results save folder\n",
        "  root = 'DCGAN_results/'\n",
        "  model = 'DCGAN_'\n",
        "  if not os.path.isdir(root):\n",
        "      os.mkdir(root)\n",
        "  if not os.path.isdir(root + 'Fixed_results'):\n",
        "      os.mkdir(root + 'Fixed_results')\n",
        "\n",
        "  train_hist = {}\n",
        "  train_hist['D_losses'] = []\n",
        "  train_hist['G_losses'] = []\n",
        "  train_hist['per_epoch_ptimes'] = []\n",
        "  train_hist['total_ptime'] = []\n",
        "\n",
        "  # training-loop\n",
        "  np.random.seed(int(time.time()))\n",
        "  print('training start!')\n",
        "  start_time = time.time()\n",
        "  for epoch in range(train_epoch):\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "    epoch_start_time = time.time()\n",
        "    #dataset.shuffle(buffer_size=dataset_length)\n",
        "    #dataset = dataset.batch(batch_size)\n",
        "    #iterator = dataset.make_one_shot_iterator()\n",
        "\n",
        "    # TODO: include progress bar\n",
        "    # to avoid overfitting, the images are shuffled every epoch\n",
        "    # if fake images are included, suffle an index list to access the corresponding label along with the image\n",
        "    np.random.shuffle(imgs)\n",
        "    for iteration in range(dataset_length // batch_size):\n",
        "      #x_ = iterator.get_next()\n",
        "      #x_ = x_.eval()\n",
        "\n",
        "      # update discriminator\n",
        "      x_ = imgs[iteration*batch_size:(iteration+1)*batch_size]\n",
        "      random_startx = randint(0, downsample_size-image_size)\n",
        "      random_starty = randint(0, downsample_size-image_size)\n",
        "      # draw a random patch from the input\n",
        "      x_ = x_[:, random_startx:random_startx+image_size, random_starty:random_starty+image_size]\n",
        "      for i in range(2):\n",
        "        if randint(0, 1):\n",
        "          x_ = np.flip(x_, axis=i+1)  # randomly flip x- and y-axis\n",
        "      z_ = np.random.normal(0, 1, (batch_size, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "\n",
        "      loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, isTrain: True})\n",
        "      D_losses.append(loss_d_)\n",
        "\n",
        "      # update generator\n",
        "      z_ = np.random.normal(0, 1, (batch_size, image_size//gen_res, image_size//gen_res, gen_start_features))\n",
        "      loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, isTrain: True})\n",
        "      G_losses.append(loss_g_)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n",
        "    fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n",
        "    show_result((epoch + 1), save=True, show=False, path=fixed_p)\n",
        "    train_hist['D_losses'].append(np.mean(D_losses))\n",
        "    train_hist['G_losses'].append(np.mean(G_losses))\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "  end_time = time.time()\n",
        "  total_ptime = end_time - start_time\n",
        "  train_hist['total_ptime'].append(total_ptime)\n",
        "\n",
        "  print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n",
        "  print(\"Training finish!... save training results\")\n",
        "  with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
        "      pickle.dump(train_hist, f)\n",
        "\n",
        "  show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
        "\n",
        "  # this thing makes a gif out of the results after every epoch, so it's not\n",
        "  # necessary \n",
        "  try:\n",
        "    import imageio\n",
        "    images = []\n",
        "    for e in range(train_epoch):\n",
        "        img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
        "        images.append(imageio.imread(img_name))\n",
        "    imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)\n",
        "  except Exception as e:\n",
        "    print(\"Haven't gotten imageio to work on the cluster, smh. Error message:\")\n",
        "    print(e)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-913e34c4cf18>:43: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /home/tillns/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-2-913e34c4cf18>:47: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "training start!\n",
            "[1/200] - ptime: 2.10 loss_d: 1.410, loss_g: 0.499\n",
            "[2/200] - ptime: 0.83 loss_d: 1.328, loss_g: 0.549\n",
            "[3/200] - ptime: 0.89 loss_d: 1.279, loss_g: 0.587\n",
            "[4/200] - ptime: 0.88 loss_d: 1.256, loss_g: 0.608\n",
            "[5/200] - ptime: 0.91 loss_d: 1.238, loss_g: 0.622\n",
            "[6/200] - ptime: 0.85 loss_d: 1.217, loss_g: 0.631\n",
            "[7/200] - ptime: 0.88 loss_d: 1.205, loss_g: 0.637\n",
            "[8/200] - ptime: 0.88 loss_d: 1.190, loss_g: 0.641\n",
            "[9/200] - ptime: 0.87 loss_d: 1.177, loss_g: 0.645\n",
            "[10/200] - ptime: 0.91 loss_d: 1.162, loss_g: 0.648\n",
            "[11/200] - ptime: 0.88 loss_d: 1.149, loss_g: 0.651\n",
            "[12/200] - ptime: 0.87 loss_d: 1.139, loss_g: 0.653\n",
            "[13/200] - ptime: 0.87 loss_d: 1.128, loss_g: 0.656\n",
            "[14/200] - ptime: 0.89 loss_d: 1.119, loss_g: 0.659\n",
            "[15/200] - ptime: 0.87 loss_d: 1.110, loss_g: 0.661\n",
            "[16/200] - ptime: 0.87 loss_d: 1.104, loss_g: 0.662\n",
            "[17/200] - ptime: 0.89 loss_d: 1.097, loss_g: 0.664\n",
            "[18/200] - ptime: 0.87 loss_d: 1.094, loss_g: 0.662\n",
            "[19/200] - ptime: 0.87 loss_d: 1.094, loss_g: 0.663\n",
            "[20/200] - ptime: 0.87 loss_d: 1.091, loss_g: 0.663\n",
            "[21/200] - ptime: 0.87 loss_d: 1.096, loss_g: 0.659\n",
            "[22/200] - ptime: 0.88 loss_d: 1.074, loss_g: 0.674\n",
            "[23/200] - ptime: 0.87 loss_d: 1.070, loss_g: 0.674\n",
            "[24/200] - ptime: 0.87 loss_d: 1.069, loss_g: 0.673\n",
            "[25/200] - ptime: 0.87 loss_d: 1.061, loss_g: 0.676\n",
            "[26/200] - ptime: 0.87 loss_d: 1.056, loss_g: 0.678\n",
            "[27/200] - ptime: 0.87 loss_d: 1.053, loss_g: 0.679\n",
            "[28/200] - ptime: 0.89 loss_d: 1.050, loss_g: 0.680\n",
            "[29/200] - ptime: 0.88 loss_d: 1.047, loss_g: 0.681\n",
            "[30/200] - ptime: 0.89 loss_d: 1.045, loss_g: 0.682\n",
            "[31/200] - ptime: 0.88 loss_d: 1.044, loss_g: 0.682\n",
            "[32/200] - ptime: 0.89 loss_d: 1.044, loss_g: 0.683\n",
            "[33/200] - ptime: 0.87 loss_d: 1.040, loss_g: 0.683\n",
            "[34/200] - ptime: 0.87 loss_d: 1.041, loss_g: 0.682\n",
            "[35/200] - ptime: 0.89 loss_d: 1.038, loss_g: 0.683\n",
            "[36/200] - ptime: 0.87 loss_d: 1.038, loss_g: 0.684\n",
            "[37/200] - ptime: 0.89 loss_d: 1.037, loss_g: 0.684\n",
            "[38/200] - ptime: 0.88 loss_d: 1.035, loss_g: 0.685\n",
            "[39/200] - ptime: 0.88 loss_d: 1.035, loss_g: 0.685\n",
            "[40/200] - ptime: 0.88 loss_d: 1.034, loss_g: 0.685\n",
            "[41/200] - ptime: 0.88 loss_d: 1.033, loss_g: 0.686\n",
            "[42/200] - ptime: 0.87 loss_d: 1.032, loss_g: 0.686\n",
            "[43/200] - ptime: 0.88 loss_d: 1.032, loss_g: 0.686\n",
            "[44/200] - ptime: 0.88 loss_d: 1.030, loss_g: 0.686\n",
            "[45/200] - ptime: 0.88 loss_d: 1.031, loss_g: 0.686\n",
            "[46/200] - ptime: 0.87 loss_d: 1.029, loss_g: 0.687\n",
            "[47/200] - ptime: 0.87 loss_d: 1.028, loss_g: 0.687\n",
            "[48/200] - ptime: 0.87 loss_d: 1.028, loss_g: 0.687\n",
            "[49/200] - ptime: 0.88 loss_d: 1.027, loss_g: 0.687\n",
            "[50/200] - ptime: 0.90 loss_d: 1.029, loss_g: 0.687\n",
            "[51/200] - ptime: 0.88 loss_d: 1.026, loss_g: 0.688\n",
            "[52/200] - ptime: 0.88 loss_d: 1.025, loss_g: 0.688\n",
            "[53/200] - ptime: 0.89 loss_d: 1.026, loss_g: 0.688\n",
            "[54/200] - ptime: 0.88 loss_d: 1.025, loss_g: 0.688\n",
            "[55/200] - ptime: 0.88 loss_d: 1.025, loss_g: 0.688\n",
            "[56/200] - ptime: 0.88 loss_d: 1.023, loss_g: 0.689\n",
            "[57/200] - ptime: 0.88 loss_d: 1.024, loss_g: 0.689\n",
            "[58/200] - ptime: 0.91 loss_d: 1.024, loss_g: 0.689\n",
            "[59/200] - ptime: 0.88 loss_d: 1.022, loss_g: 0.689\n",
            "[60/200] - ptime: 0.88 loss_d: 1.022, loss_g: 0.689\n",
            "[61/200] - ptime: 0.89 loss_d: 1.022, loss_g: 0.689\n",
            "[62/200] - ptime: 0.88 loss_d: 1.021, loss_g: 0.689\n",
            "[63/200] - ptime: 0.87 loss_d: 1.021, loss_g: 0.689\n",
            "[64/200] - ptime: 0.90 loss_d: 1.020, loss_g: 0.689\n",
            "[65/200] - ptime: 0.89 loss_d: 1.020, loss_g: 0.690\n",
            "[66/200] - ptime: 0.87 loss_d: 1.019, loss_g: 0.690\n",
            "[67/200] - ptime: 0.91 loss_d: 1.019, loss_g: 0.690\n",
            "[68/200] - ptime: 0.84 loss_d: 1.019, loss_g: 0.690\n",
            "[69/200] - ptime: 0.89 loss_d: 1.019, loss_g: 0.690\n",
            "[70/200] - ptime: 0.87 loss_d: 1.019, loss_g: 0.690\n",
            "[71/200] - ptime: 0.88 loss_d: 1.018, loss_g: 0.690\n",
            "[72/200] - ptime: 0.90 loss_d: 1.017, loss_g: 0.690\n",
            "[73/200] - ptime: 0.87 loss_d: 1.017, loss_g: 0.690\n",
            "[74/200] - ptime: 0.88 loss_d: 1.018, loss_g: 0.690\n",
            "[75/200] - ptime: 0.88 loss_d: 1.017, loss_g: 0.690\n",
            "[76/200] - ptime: 0.88 loss_d: 1.017, loss_g: 0.690\n",
            "[77/200] - ptime: 0.90 loss_d: 1.017, loss_g: 0.690\n",
            "[78/200] - ptime: 0.85 loss_d: 1.016, loss_g: 0.690\n",
            "[79/200] - ptime: 0.88 loss_d: 1.016, loss_g: 0.690\n",
            "[80/200] - ptime: 0.88 loss_d: 1.016, loss_g: 0.690\n",
            "[81/200] - ptime: 0.88 loss_d: 1.015, loss_g: 0.690\n",
            "[82/200] - ptime: 0.90 loss_d: 1.015, loss_g: 0.690\n",
            "[83/200] - ptime: 0.87 loss_d: 1.019, loss_g: 0.690\n",
            "[84/200] - ptime: 0.88 loss_d: 1.017, loss_g: 0.690\n",
            "[85/200] - ptime: 0.88 loss_d: 1.016, loss_g: 0.690\n",
            "[86/200] - ptime: 0.88 loss_d: 1.016, loss_g: 0.690\n",
            "[87/200] - ptime: 0.87 loss_d: 1.016, loss_g: 0.690\n",
            "[88/200] - ptime: 0.87 loss_d: 1.016, loss_g: 0.690\n",
            "[89/200] - ptime: 0.88 loss_d: 1.015, loss_g: 0.691\n",
            "[90/200] - ptime: 0.87 loss_d: 1.016, loss_g: 0.691\n",
            "[91/200] - ptime: 0.87 loss_d: 1.017, loss_g: 0.691\n",
            "[92/200] - ptime: 0.87 loss_d: 1.016, loss_g: 0.690\n",
            "[93/200] - ptime: 0.89 loss_d: 1.015, loss_g: 0.691\n",
            "[94/200] - ptime: 0.87 loss_d: 1.016, loss_g: 0.691\n",
            "[95/200] - ptime: 0.88 loss_d: 1.016, loss_g: 0.691\n",
            "[96/200] - ptime: 0.88 loss_d: 1.016, loss_g: 0.691\n",
            "[97/200] - ptime: 0.88 loss_d: 1.015, loss_g: 0.691\n",
            "[98/200] - ptime: 0.88 loss_d: 1.015, loss_g: 0.691\n",
            "[99/200] - ptime: 0.89 loss_d: 1.015, loss_g: 0.691\n",
            "[100/200] - ptime: 0.88 loss_d: 1.014, loss_g: 0.691\n",
            "[101/200] - ptime: 0.89 loss_d: 1.014, loss_g: 0.691\n",
            "[102/200] - ptime: 0.87 loss_d: 1.014, loss_g: 0.691\n",
            "[103/200] - ptime: 0.89 loss_d: 1.014, loss_g: 0.691\n",
            "[104/200] - ptime: 0.87 loss_d: 1.014, loss_g: 0.691\n",
            "[105/200] - ptime: 0.87 loss_d: 1.013, loss_g: 0.691\n",
            "[106/200] - ptime: 0.88 loss_d: 1.013, loss_g: 0.691\n",
            "[107/200] - ptime: 0.88 loss_d: 1.013, loss_g: 0.691\n",
            "[108/200] - ptime: 0.88 loss_d: 1.013, loss_g: 0.691\n",
            "[109/200] - ptime: 0.88 loss_d: 1.013, loss_g: 0.691\n",
            "[110/200] - ptime: 0.88 loss_d: 1.012, loss_g: 0.691\n",
            "[111/200] - ptime: 0.89 loss_d: 1.012, loss_g: 0.691\n",
            "[112/200] - ptime: 0.87 loss_d: 1.012, loss_g: 0.691\n",
            "[113/200] - ptime: 0.88 loss_d: 1.012, loss_g: 0.691\n",
            "[114/200] - ptime: 0.88 loss_d: 1.012, loss_g: 0.691\n",
            "[115/200] - ptime: 0.88 loss_d: 1.012, loss_g: 0.691\n",
            "[116/200] - ptime: 0.87 loss_d: 1.012, loss_g: 0.692\n",
            "[117/200] - ptime: 0.89 loss_d: 1.011, loss_g: 0.692\n",
            "[118/200] - ptime: 0.89 loss_d: 1.011, loss_g: 0.692\n",
            "[119/200] - ptime: 0.91 loss_d: 1.012, loss_g: 0.691\n",
            "[120/200] - ptime: 0.90 loss_d: 1.012, loss_g: 0.691\n",
            "[121/200] - ptime: 0.85 loss_d: 1.011, loss_g: 0.692\n",
            "[122/200] - ptime: 0.88 loss_d: 1.011, loss_g: 0.692\n",
            "[123/200] - ptime: 0.87 loss_d: 1.011, loss_g: 0.692\n",
            "[124/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[125/200] - ptime: 0.89 loss_d: 1.010, loss_g: 0.692\n",
            "[126/200] - ptime: 0.93 loss_d: 1.011, loss_g: 0.692\n",
            "[127/200] - ptime: 0.82 loss_d: 1.011, loss_g: 0.692\n",
            "[128/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[129/200] - ptime: 0.88 loss_d: 1.011, loss_g: 0.692\n",
            "[130/200] - ptime: 0.86 loss_d: 1.010, loss_g: 0.692\n",
            "[131/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[132/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[133/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[134/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[135/200] - ptime: 0.89 loss_d: 1.010, loss_g: 0.692\n",
            "[136/200] - ptime: 0.90 loss_d: 1.010, loss_g: 0.692\n",
            "[137/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[138/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[139/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[140/200] - ptime: 0.87 loss_d: 1.020, loss_g: 0.691\n",
            "[141/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[142/200] - ptime: 0.88 loss_d: 1.010, loss_g: 0.692\n",
            "[143/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[144/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[145/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[146/200] - ptime: 0.88 loss_d: 1.011, loss_g: 0.692\n",
            "[147/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[148/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[149/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[150/200] - ptime: 0.86 loss_d: 1.009, loss_g: 0.692\n",
            "[151/200] - ptime: 0.88 loss_d: 1.009, loss_g: 0.692\n",
            "[152/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[153/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[154/200] - ptime: 0.88 loss_d: 1.009, loss_g: 0.692\n",
            "[155/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[156/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[157/200] - ptime: 0.87 loss_d: 1.010, loss_g: 0.692\n",
            "[158/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[159/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[160/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[161/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[162/200] - ptime: 0.88 loss_d: 1.010, loss_g: 0.692\n",
            "[163/200] - ptime: 0.88 loss_d: 1.009, loss_g: 0.692\n",
            "[164/200] - ptime: 0.89 loss_d: 1.009, loss_g: 0.692\n",
            "[165/200] - ptime: 0.91 loss_d: 1.009, loss_g: 0.692\n",
            "[166/200] - ptime: 0.86 loss_d: 1.009, loss_g: 0.692\n",
            "[167/200] - ptime: 0.88 loss_d: 1.009, loss_g: 0.692\n",
            "[168/200] - ptime: 0.89 loss_d: 1.009, loss_g: 0.692\n",
            "[169/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[170/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[171/200] - ptime: 0.89 loss_d: 1.012, loss_g: 0.692\n",
            "[172/200] - ptime: 0.87 loss_d: 1.012, loss_g: 0.693\n",
            "[173/200] - ptime: 0.89 loss_d: 1.010, loss_g: 0.692\n",
            "[174/200] - ptime: 0.86 loss_d: 1.009, loss_g: 0.692\n",
            "[175/200] - ptime: 0.88 loss_d: 1.009, loss_g: 0.691\n",
            "[176/200] - ptime: 0.88 loss_d: 1.010, loss_g: 0.691\n",
            "[177/200] - ptime: 0.89 loss_d: 1.009, loss_g: 0.691\n",
            "[178/200] - ptime: 0.90 loss_d: 1.009, loss_g: 0.692\n",
            "[179/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[180/200] - ptime: 0.88 loss_d: 1.009, loss_g: 0.692\n",
            "[181/200] - ptime: 0.90 loss_d: 1.009, loss_g: 0.692\n",
            "[182/200] - ptime: 0.90 loss_d: 1.009, loss_g: 0.692\n",
            "[183/200] - ptime: 0.85 loss_d: 1.009, loss_g: 0.692\n",
            "[184/200] - ptime: 0.87 loss_d: 1.009, loss_g: 0.692\n",
            "[185/200] - ptime: 0.90 loss_d: 1.009, loss_g: 0.692\n",
            "[186/200] - ptime: 0.90 loss_d: 1.012, loss_g: 0.693\n",
            "[187/200] - ptime: 0.85 loss_d: 1.009, loss_g: 0.693\n",
            "[188/200] - ptime: 0.89 loss_d: 1.020, loss_g: 0.693\n",
            "[189/200] - ptime: 0.85 loss_d: 1.007, loss_g: 0.693\n",
            "[190/200] - ptime: 0.88 loss_d: 1.007, loss_g: 0.693\n",
            "[191/200] - ptime: 0.89 loss_d: 1.007, loss_g: 0.693\n",
            "[192/200] - ptime: 0.89 loss_d: 1.007, loss_g: 0.693\n",
            "[193/200] - ptime: 0.88 loss_d: 1.007, loss_g: 0.693\n",
            "[194/200] - ptime: 0.87 loss_d: 1.007, loss_g: 0.693\n",
            "[195/200] - ptime: 0.89 loss_d: 1.007, loss_g: 0.693\n",
            "[196/200] - ptime: 0.88 loss_d: 1.007, loss_g: 0.693\n",
            "[197/200] - ptime: 0.89 loss_d: 1.007, loss_g: 0.693\n",
            "[198/200] - ptime: 0.90 loss_d: 1.007, loss_g: 0.693\n",
            "[199/200] - ptime: 0.88 loss_d: 1.007, loss_g: 0.693\n",
            "[200/200] - ptime: 0.87 loss_d: 1.007, loss_g: 0.693\n",
            "Avg per epoch ptime: 0.88, total 200 epochs ptime: 291.83\n",
            "Training finish!... save training results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oOrQyfQyz8ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*# Load Dataset*"
      ]
    },
    {
      "metadata": {
        "id": "L-QHIAkMkA39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "suMOrosIj8Br",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}